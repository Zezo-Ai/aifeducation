#'@title Text embedding classifier with a neural net
#'
#'@description Abstract class for neural nets with 'keras'/'tensorflow' and
#''pytorch'.
#'
#'@return Objects of this class are used for assigning texts to classes/categories. For
#'the creation and training of a classifier an object of class \link{EmbeddedText} and a \code{factor}
#'are necessary. The object of class \link{EmbeddedText} contains the numerical text
#'representations (text embeddings) of the raw texts generated by an object of class
#'\link{TextEmbeddingModel}. The \code{factor} contains the classes/categories for every
#'text. Missing values (unlabeled cases) are supported. For predictions an object of class
#'\link{EmbeddedText} has to be used which was created with the same text embedding model as
#'for training.
#'@family Classification
#'@export
TextEmbeddingClassifierNeuralNet<-R6::R6Class(
  classname = "TextEmbeddingClassifierNeuralNet",
  public = list(
    #'@field model ('tensorflow_model()')\cr
    #'Field for storing the tensorflow model after loading.
    model=NULL,

    #'@field model_config ('list()')\cr
    #'List for storing information about the configuration of the model. This
    #'information is used to predict new data.
    #'\itemize{
    #'\item{\code{model_config$n_rec: }Number of recurrent layers.}
    #'\item{\code{model_config$n_hidden: }Number of dense layers.}
    #'\item{\code{model_config$target_levels: }Levels of the target variable. Do not change this manually.}
    #'\item{\code{model_config$input_variables: }Order and name of the input variables. Do not change this manually.}
    #'\item{\code{model_config$init_config: }List storing all parameters passed to method new().}
    #'}
    model_config=list(),

    #'@field last_training ('list()')\cr
    #'List for storing the history and the results of the last training. This
    #'information will be overwritten if a new training is started.
    #'\itemize{
    #'\item{\code{last_training$learning_time: }Duration of the training process.}
    #'\item{\code{config$history: }History of the last training.}
    #'\item{\code{config$data: }Object of class table storing the initial frequencies of the passed data.}
    #'\item{\code{config$data_pb:l }Matrix storing the number of additional cases (test and training) added
    #'during balanced pseudo-labeling. The rows refer to folds and final training.
    #'The columns refer to the steps during pseudo-labeling.}
    #'\item{\code{config$data_bsc_test: }Matrix storing the number of cases for each category used for testing
    #'during the phase of balanced synthetic units. Please note that the
    #'frequencies include original and synthetic cases. In case the number
    #'of original and synthetic cases exceeds the limit for the majority classes,
    #'the frequency represents the number of cases created by cluster analysis.}
    #'\item{\code{config$date: }Time when the last training finished.}
    #'\item{\code{config$config: }List storing which kind of estimation was requested during the last training.
    #'\itemize{
    #'\item{\code{config$config$use_bsc:  }\code{TRUE} if  balanced synthetic cases were requested. \code{FALSE}
    #'if not.}
    #'\item{\code{config$config$use_baseline: }\code{TRUE} if baseline estimation were requested. \code{FALSE}
    #'if not.}
    #'\item{\code{ config$config$use_bpl: }\code{TRUE} if  balanced, pseudo-labeling cases were requested. \code{FALSE}
    #'if not.}
    #'}}
    #'}
    last_training=list(
      learning_time=NULL,
      history=list(
        bsl=NULL,
        bsc=NULL,
        bpl=NULL
      ),
      data=NULL,
      data_pbl=NULL,
      data_bsc_train=NULL,
      data_bsc_test=NULL,
      date=NULL,
      n_samples=NULL,
      config=list(
        use_bsc=NULL,
        use_baseline=NULL,
        use_bpl=NULL
      )
    ),

    #'@field reliability ('list()')\cr
    #'List for storing central reliability measures of the last training.
    #'\itemize{
    #'\item{\code{reliability$test_metric: }Array containing the reliability measures for the validation data for
    #'every fold, method, and step (in case of pseudo-labeling).}
    #'\item{\code{reliability$test_metric_mean: }Array containing the reliability measures for the validation data for
    #'every method and step (in case of pseudo-labeling). The values represent
    #'the mean values for every fold.}
    #'\item{\code{reliability$raw_iota_objects: }List containing all iota_object generated with the package \code{iotarelr}
    #'for every fold at the start and the end of the last training.
    #'\itemize{
    #'\item{\code{reliability$raw_iota_objects$iota_objects_start: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the baseline model
    #'for every fold.
    #'If the estimation of the baseline model is not requested, the list is
    #'set to \code{NULL}.}
    #'\item{\code{reliability$raw_iota_objects$iota_objects_end: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the final model
    #'for every fold. Depending of the requested training method these values
    #'refer to the baseline model, a trained model on the basis of balanced
    #'synthetic cases, balanced pseudo labeling or a combination of balanced
    #'synthetic cases with pseudo labeling.}
    #'\item{\code{reliability$raw_iota_objects$iota_objects_start_free: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the baseline model
    #'for every fold.
    #'If the estimation of the baseline model is not requested, the list is
    #'set to \code{NULL}.Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'\item{\code{reliability$raw_iota_objects$iota_objects_end_free: }List of objects with class \code{iotarelr_iota2} containing the
    #'estimated iota reliability of the second generation for the final model
    #'for every fold. Depending of the requested training method, these values
    #'refer to the baseline model, a trained model on the basis of balanced
    #'synthetic cases, balanced pseudo-labeling or a combination of balanced
    #'synthetic cases and pseudo-labeling.
    #'Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'}
    #'}
    #'\item{\code{reliability$iota_object_start: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold. If the estimation of the baseline model is not requested, the list is
    #'set to \code{NULL}.}
    #'\item{\code{ reliability$iota_object_start_free: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold. If the estimation of the baseline model is not requested, the list is
    #'set to \code{NULL}.
    #'Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'\item{\code{reliability$iota_object_end: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold.
    #'Depending on the requested training method, this object
    #'refers to the baseline model, a trained model on the basis of balanced
    #'synthetic cases, balanced pseudo-labeling or a combination of balanced
    #'synthetic cases and pseudo-labeling.}
    #'\item{\code{reliability$iota_object_end_free: }Object of class \code{iotarelr_iota2} as a mean of the individual objects
    #'for every fold.
    #'Depending on the requested training method, this object
    #'refers to the baseline model, a trained model on the basis of balanced
    #'synthetic cases, balanced pseudo-labeling or a combination of balanced
    #'synthetic cases and pseudo-labeling.
    #'Please note that the model is estimated without
    #'forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.}
    #'\item{\code{reliability$standard_measures_end: }Object of class \code{list} containing the final
    #'measures for precision, recall, and f1 for every fold.
    #'Depending of the requested training method, these values
    #'refer to the baseline model, a trained model on the basis of balanced
    #'synthetic cases, balanced pseudo-labeling or a combination of balanced
    #'synthetic cases and pseudo-labeling.}
    #'\item{\code{reliability$standard_measures_mean: }\code{matrix} containing the mean
    #'measures for precision, recall, and f1 at the end of every fold.}
    #'}
    #'
    reliability=list(
      test_metric=NULL,
      test_metric_mean=NULL,
      raw_iota_objects=list(
        iota_objects_start=NULL,
        iota_objects_end=NULL,
        iota_objects_start_free=NULL,
        iota_objects_end_free=NULL),
      iota_object_start=NULL,
      iota_object_start_free=NULL,
      iota_object_end=NULL,
      iota_object_end_free=NULL,
      standard_measures_end=NULL,
      standard_measures_mean=NULL
    ),

    #New-----------------------------------------------------------------------
    #'@description Creating a new instance of this class.
    #'@param ml_framework \code{string} Framework to use for training and inference.
    #'\code{ml_framework="tensorflow"} for 'tensorflow' and \code{ml_framework="pytorch"}
    #'for 'pytorch'
    #'@param name \code{Character} Name of the new classifier. Please refer to
    #'common name conventions. Free text can be used with parameter \code{label}.
    #'@param label \code{Character} Label for the new classifier. Here you can use
    #'free text.
    #'@param text_embeddings An object of class\code{TextEmbeddingModel}.
    #'@param targets \code{factor} containing the target values of the classifier.
    #'@param hidden \code{vector} containing the number of neurons for each dense layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param rec \code{vector} containing the number of neurons for each recurrent layer.
    #'The length of the vector determines the number of dense layers. If you want no dense layer,
    #'set this parameter to \code{NULL}.
    #'@param attention_type \code{string} Choose the relevant attention type. Possible values
    #'are \code{"fourier"} and \code{multihead}.
    #'@param self_attention_heads \code{integer} determining the number of attention heads
    #'for a self-attention layer. Only relevant if \code{attention_type="multihead"}
    #'@param repeat_encoder \code{int} determining how many times the encoder should be
    #'added to the network.
    #'@param intermediate_size \code{int} determining the size of the projection layer within
    #'a each transformer encoder.
    #'@param add_pos_embedding \code{bool} \code{TRUE} if positional embedding should be used.
    #'@param encoder_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout for the dense projection within the encoder layers.
    #'@param dense_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between dense layers.
    #'@param rec_dropout \code{double} ranging between 0 and lower 1, determining the
    #'dropout between bidirectional gru layers.
    #'@param recurrent_dropout \code{double} ranging between 0 and lower 1, determining the
    #'recurrent dropout for each recurrent layer. Only relevant for keras models.
    #'@param optimizer Object of class \code{keras.optimizers}.
    #'@return Returns an object of class \link{TextEmbeddingClassifierNeuralNet} which is ready for
    #'training.
    initialize=function(ml_framework=aifeducation_config$get_framework(),
                        name=NULL,
                        label=NULL,
                        text_embeddings=NULL,
                        targets=NULL,
                        hidden=c(128),
                        rec=c(128),
                        self_attention_heads=0,
                        intermediate_size=NULL,
                        attention_type="fourier",
                        add_pos_embedding=TRUE,
                        rec_dropout=0.1,
                        repeat_encoder=1,
                        dense_dropout=0.4,
                        recurrent_dropout=0.4,
                        encoder_dropout=0.1,
                        optimizer="adam"
    ){
      #Checking of parameters--------------------------------------------------
      if(is.null(name)){
        stop("name is NULL but must be a character.")
      }
      if(is.null(label)){
        stop("label is NULL but must be a character.")
      }
      if(!("EmbeddedText" %in% class(text_embeddings))){
        stop("text_embeddings must be of class EmbeddedText.")
      }
      if(is.factor(targets)==FALSE){
        stop("targets must be of class factor.")
      }

      if(!(is.numeric(hidden)==TRUE | is.null(hidden)==TRUE)){
        stop("hidden must be a vector of integer or NULL.")
      }
      if(!(is.numeric(rec)==TRUE | is.null(rec)==TRUE)){
        stop("rec must be a vector of integer or NULL.")
      }
      if(is.integer(as.integer(self_attention_heads))==FALSE){
        stop("self_attention_heads must be an integer.")
      }

      if(optimizer %in% c("adam","rmsprop")==FALSE){
        stop("Optimzier must be 'adam' oder 'rmsprop'.")
      }

      if(attention_type %in% c("fourier","multihead")==FALSE){
        stop("Optimzier must be 'fourier' oder 'multihead'.")
      }
      if(repeat_encoder>0 & attention_type=="multihead" & self_attention_heads<=0){
        stop("Encoder layer is set to 'multihead'. This requires self_attention_heads>=1.")
      }

      #------------------------------------------------------------------------

      #Setting ML Framework
      if((ml_framework %in% c("tensorflow","pytorch"))==FALSE) {
        stop("ml_framework must be 'tensorflow' or 'pytorch'.")
      }

      private$ml_framework=ml_framework

      #Setting Label and Name-------------------------------------------------
      private$model_info$model_name_root=name
      private$model_info$model_name=paste0(private$model_info$model_name_root,"_ID_",generate_id(16))
      private$model_info$model_label=label

      #Basic Information of Input and Target Data
      variable_name_order<-dimnames(text_embeddings$embeddings)[[3]]
      target_levels_order<-levels(targets)

      model_info=text_embeddings$get_model_info()
      times=model_info$param_chunks
      features=dim(text_embeddings$embeddings)[3]

      private$text_embedding_model["model"]=list(model_info)
      private$text_embedding_model["times"]=times
      private$text_embedding_model["features"]=features

      if(is.null(rec) & self_attention_heads>0){
        if(features %% 2 !=0){
          stop("The number of features of the TextEmbeddingmodel is
               not a multiple of 2.")
        }
      }

      if(is.null(intermediate_size)==TRUE){
        if(attention_type=="fourier" & length(rec)>0){
          intermediate_size=2*rec[length(rec)]
        } else if(attention_type=="fourier" & length(rec)==0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)>0 & self_attention_heads>0){
          intermediate_size=2*features
        } else if(attention_type=="multihead" & length(rec)==0 & self_attention_heads>0){
          intermediate_size=2*features
        } else {
          intermediate_size=NULL
        }
      }

      #Saving Configuration
      config=list(
        features= private$text_embedding_model[["features"]],
        times=private$text_embedding_model[["times"]],
        hidden=hidden,
        rec=rec,
        intermediate_size=intermediate_size,
        attention_type=attention_type,
        repeat_encoder=repeat_encoder,
        dense_dropout=dense_dropout,
        rec_dropout=rec_dropout,
        recurrent_dropout=recurrent_dropout,
        encoder_dropout=encoder_dropout,
        add_pos_embedding=add_pos_embedding,
        optimizer=optimizer,
        act_fct="gelu",
        rec_act_fct="tanh",
        self_attention_heads=self_attention_heads)

      if(length(target_levels_order)>2){
        #Multi Class
        config["act_fct_last"]="softmax"
        config["err_fct"]="categorical_crossentropy"
        config["metric"]="categorical_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      } else {
        #Binary Classification
        config["act_fct_last"]="sigmoid"
        config["err_fct"]="binary_crossentropy"
        config["metric"]="binary_accuracy"
        config["balanced_metric"]="balanced_accuracy"
      }

      config["target_levels"]=list(target_levels_order)
      config["input_variables"]=list(variable_name_order)

      self$model_config=config

      #Create_Model------------------------------------------------------------
      if(private$ml_framework=="tensorflow"){
        self$model=private$create_model_keras(
          features=self$model_config$features ,
          times=self$model_config$times,
          hidden=self$model_config$hidden,
          rec=self$model_config$rec,
          embed_dim=self$model_config$embed_dim,
          intermediate_size=self$model_config$intermediate_size,
          attention_type=self$model_config$attention_type,
          repeat_encoder=self$model_config$repeat_encoder,
          dense_dropout=self$model_config$dense_dropout,
          rec_dropout=self$model_config$rec_dropout,
          recurrent_dropout=self$model_config$recurrent_dropout,
          encoder_dropout=self$model_config$encoder_dropout,
          add_pos_embedding=self$model_config$add_pos_embedding,
          self_attention_heads=self$model_config$self_attention_heads,
          target_levels=self$model_config$target_levels,
          act_fct_last=self$model_config$act_fct_last,
          name=private$model_info$model_name_root
        )
      } else if (private$ml_framework=="pytorch"){
        self$model=private$create_model_pytorch(
          features=self$model_config$features ,
          times=self$model_config$times,
          hidden=self$model_config$hidden,
          rec=self$model_config$rec,
          intermediate_size=self$model_config$intermediate_size,
          attention_type=self$model_config$attention_type,
          repeat_encoder=self$model_config$repeat_encoder,
          dense_dropout=self$model_config$dense_dropout,
          rec_dropout=self$model_config$rec_dropout,
          encoder_dropout=self$model_config$encoder_dropout,
          add_pos_embedding=self$model_config$add_pos_embedding,
          self_attention_heads=self$model_config$self_attention_heads,
          target_levels=self$model_config$target_levels
        )
      }

      private$model_info$model_date=date()

      private$r_package_versions$aifeducation<-packageVersion("aifeducation")
      private$r_package_versions$reticulate<-packageVersion("reticulate")
      private$r_package_versions$smotefamily<-packageVersion("smotefamily")

      private$py_package_versions$tensorflow<-tf$version$VERSION
      private$py_package_versions$torch<-torch["__version__"]
      private$py_package_versions$keras<-keras["__version__"]
      private$py_package_versions$numpy<-np$version$short_version
    },

    #-------------------------------------------------------------------------
    #'@description Method for training a neural net.
    #'@param data_embeddings Object of class \code{TextEmbeddingModel}.
    #'@param data_targets \code{Factor} containing the labels for cases
    #'stored in \code{data_embeddings}. Factor must be named and has to use the
    #'same names used in \code{data_embeddings}.
    #'@param data_n_test_samples \code{int} determining the number of cross-fold
    #'samples.
    #'@param balance_class_weights \code{bool} If \code{TRUE} class weights are
    #'generated based on the frequencies of the training data with the method
    #'Inverse Class Frequency'. If \code{FALSE} each class has the weight 1.
    #'@param use_baseline \code{bool} \code{TRUE} if the calculation of a baseline
    #'model is requested. This option is only relevant for \code{use_bsc=TRUE} or
    #'\code{use_pbl=TRUE}. If both are \code{FALSE}, a baseline model is calculated.
    #'@param bsl_val_size \code{double} between 0 and 1, indicating the proportion of cases of each class
    #'which should be used for the validation sample during the estimation of the baseline model.
    #'The remaining cases are part of the training data.
    #'@param use_bsc \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced synthetic cases. \code{FALSE} if not.
    #'@param bsc_methods \code{vector} containing the methods for generating
    #'synthetic cases via 'smotefamily'. Multiple methods can
    #'be passed. Currently \code{bsc_methods=c("adas")}, \code{bsc_methods=c("smote")}
    #'and \code{bsc_methods=c("dbsmote")} are possible.
    #'@param bsc_max_k \code{int} determining the maximal number of k which is used
    #'for creating synthetic units.
    #'@param bsc_val_size \code{double} between 0 and 1, indicating the proportion of cases of each class
    #'which should be used for the validation sample during the estimation with synthetic cases.
    #'@param bsc_add_all \code{bool} If \code{FALSE} only synthetic cases necessary to fill
    #'the gab between the class and the major class are added to the data. If \code{TRUE} all
    #'generated synthetic cases are added to the data.
    #'@param use_bpl \code{bool} \code{TRUE} if the estimation should integrate
    #'balanced pseudo-labeling. \code{FALSE} if not.
    #'@param bpl_max_steps \code{int} determining the maximum number of steps during
    #'pseudo-labeling.
    #'@param bpl_epochs_per_step \code{int} Number of training epochs within every step.
    #'@param bpl_model_reset \code{bool} If \code{TRUE}, model is re-initialized at every
    #'step.
    #'@param bpl_dynamic_inc \code{bool} If \code{TRUE}, only a specific percentage
    #'of cases is included during each step. The percentage is determined by
    #'\eqn{step/bpl_max_steps}. If \code{FALSE}, all cases are used.
    #'@param bpl_balance \code{bool} If \code{TRUE}, the same number of cases for
    #'every category/class of the pseudo-labeled data are used with training. That
    #'is, the number of cases is determined by the minor class/category.
    #'@param bpl_anchor \code{double} between 0 and 1 indicating the reference
    #'point for sorting the new cases of every label. See notes for more details.
    #'@param bpl_max \code{double} between 0 and 1, setting the maximal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param bpl_min \code{double} between 0 and 1, setting the minimal level of
    #'confidence for considering a case for pseudo-labeling.
    #'@param bpl_weight_inc \code{double} value how much the sample weights
    #'should be increased for the cases with pseudo-labels in every step.
    #'@param bpl_weight_start \code{dobule} Starting value for the weights of the
    #'unlabeled cases.
    #'@param sustain_track \code{bool} If \code{TRUE} energy consumption is tracked
    #'during training via the python library codecarbon.
    #'@param sustain_iso_code \code{string} ISO code (Alpha-3-Code) for the country. This variable
    #'must be set if sustainability should be tracked. A list can be found on
    #'Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.
    #'@param sustain_region Region within a country. Only available for USA and
    #'Canada See the documentation of codecarbon for more information.
    #'\url{https://mlco2.github.io/codecarbon/parameters.html}
    #'@param sustain_interval \code{integer} Interval in seconds for measuring power
    #'usage.
    #'@param epochs \code{int} Number of training epochs.
    #'@param batch_size \code{int} Size of batches.
    #'@param dir_checkpoint \code{string} Path to the directory where
    #'the checkpoint during training should be saved. If the directory does not
    #'exist, it is created.
    #'@param trace \code{bool} \code{TRUE}, if information about the estimation
    #'phase should be printed to the console.
    #'@param keras_trace \code{int} \code{keras_trace=0} does not print any
    #'information about the training process from keras on the console.
    #'@param pytorch_trace \code{int} \code{pytorch_trace=0} does not print any
    #'information about the training process from pytorch on the console.
    #'\code{pytorch_trace=1} prints a progress bar. \code{pytorch_trace=2} prints
    #'one line of information for every epoch.
    #'@param n_cores \code{int} Number of cores used for creating synthetic units.
    #'@return Function does not return a value. It changes the object into a trained
    #'classifier.
    #'@details \itemize{
    #'
    #'\item{\code{bsc_max_k: }All values from 2 up to bsc_max_k are successively used. If
    #'the number of bsc_max_k is too high, the value is reduced to a number that
    #'allows the calculating of synthetic units.}
    #'
    #'\item{\code{bpl_anchor: }With the help of this value, the new cases are sorted. For
    #'this aim, the distance from the anchor is calculated and all cases are arranged
    #'into an ascending order.
    #'}
    #'}
    #'@importFrom abind abind
    train=function(data_embeddings,
                   data_targets,
                   data_n_test_samples=5,
                   balance_class_weights=TRUE,
                   use_baseline=TRUE,
                   bsl_val_size=0.25,
                   use_bsc=TRUE,
                   bsc_methods=c("dbsmote"),
                   bsc_max_k=10,
                   bsc_val_size=0.25,
                   bsc_add_all=FALSE,
                   use_bpl=TRUE,
                   bpl_max_steps=3,
                   bpl_epochs_per_step=1,
                   bpl_dynamic_inc=FALSE,
                   bpl_balance=FALSE,
                   bpl_max=1.00,
                   bpl_anchor=1.00,
                   bpl_min=0.00,
                   bpl_weight_inc=0.02,
                   bpl_weight_start=0.00,
                   bpl_model_reset=FALSE,
                   sustain_track=TRUE,
                   sustain_iso_code=NULL,
                   sustain_region=NULL,
                   sustain_interval=15,
                   epochs=40,
                   batch_size=32,
                   dir_checkpoint,
                   trace=TRUE,
                   keras_trace=2,
                   pytorch_trace=2,
                   n_cores=2){

      #Load Custom Model Scripts
      if(private$ml_framework=="tensorflow"){
        reticulate::py_run_file(system.file("python/keras_te_classifier.py",
                                            package = "aifeducation"))
      } else if(private$ml_framework=="pytorch"){
        reticulate::py_run_file(system.file("python/pytorch_te_classifier.py",
                                            package = "aifeducation"))
      }

      #Check for a running Shiny App and set the configuration
      #The Gui functions must be set in the server function of shiny globaly
      if(require("shiny") & require("shinyWidgets")){
        if(shiny::isRunning()){
          shiny_app_active=TRUE
        } else {
          shiny_app_active=FALSE
        }
      } else {
        shiny_app_active=FALSE
      }

      requireNamespace(package="foreach")
      start_time=Sys.time()
      base::gc(verbose = FALSE,full = TRUE)

      #SetUp Progressbar for UI
      pgr_value=0
      if(use_bpl==TRUE & use_bsc==FALSE & use_baseline==FALSE){
        pgr_max_value=(data_n_test_samples+1)*(use_bpl*bpl_max_steps+use_baseline)+2
      } else {
        pgr_max_value=data_n_test_samples*(use_baseline+use_bsc+use_bpl*bpl_max_steps)+use_baseline+use_bsc+use_bpl*bpl_max_steps+1
      }
      update_aifeducation_progress_bar(value = 0,
                                       total = pgr_max_value,
                                       title = "Train Classifier")

      #Start Sustainability Tracking
      if(sustain_track==TRUE){
        if(is.null(sustain_iso_code)==TRUE){
          stop("Sustainability tracking is activated but iso code for the
               country is missing. Add iso code or deactivate tracking.")
        }
        sustainability_tracker<-codecarbon$OfflineEmissionsTracker(
          country_iso_code=sustain_iso_code,
          region=sustain_region,
          tracking_mode="machine",
          log_level="warning",
          measure_power_secs=sustain_interval,
          save_to_file=FALSE,
          save_to_api=FALSE
        )
        sustainability_tracker$start()
      }

      #Set Up Parallel Execution
      if(use_bsc==TRUE){
        cl <- parallel::makeCluster(n_cores)
        doParallel::registerDoParallel(cl)
      }

      #Checking Prerequisites
      if(trace==TRUE){
        message(paste(date(),
                    "Start"))
      }

      if(!("EmbeddedText" %in% class(data_embeddings))){
        stop("data_embeddings must be an object of class EmbeddedText")
      }

      if(self$check_embedding_model(data_embeddings)==FALSE){
        stop("The TextEmbeddingModel that generated the data_embeddings is not
               the same as the TextEmbeddingModel when generating the classifier.")
      }

      if(is.factor(data_targets)==FALSE){
        stop("data_targets must be a factor.")
      }
      if(is.null(names(data_targets))){
        stop("data_targets must be a named factor.")
      }

      if(bpl_anchor<bpl_min){
        stop("bpl_anchor must be at least bpl_min.")
      }
      if(bpl_anchor>bpl_max){
        stop("bpl_anchor must be lower or equal to bpl_max.")
      }

      if(data_n_test_samples<2){
        stop("data_n_test_samples must be at least 2.")
      }

      #------------------------------------------------------------------------
      if(trace==TRUE){
        message(paste(date(),
                    "Matching Input and Target Data"))
      }

      data_embeddings=data_embeddings$clone(deep=TRUE)
      viable_cases=base::intersect(x=rownames(data_embeddings$embeddings),
                                   names(data_targets))
      data_embeddings$embeddings=data_embeddings$embeddings[viable_cases,,,drop=FALSE]
      data_targets=data_targets[viable_cases]

      #Reducing to unique cases
      if(trace==TRUE){
        message(paste(date(),
                    "Checking Uniqueness of Data"))
      }
      n_init_cases=nrow(data_embeddings$embeddings)
      data_embeddings$embeddings=unique(data_embeddings$embeddings)
      n_final_cases=nrow(data_embeddings$embeddings)
      viable_cases=base::intersect(x=rownames(data_embeddings$embeddings),
                                   names(data_targets))
      data_embeddings$embeddings=data_embeddings$embeddings[viable_cases,,,drop=FALSE]
      data_targets=data_targets[viable_cases]
      if(trace==TRUE){
        message(paste(date(),
                    "Total Cases:",n_init_cases,
                    "Unique Cases:",n_final_cases,
                    "Labeled Cases:",length(na.omit(data_targets))))
      }


      pgr_value=pgr_value+1
      update_aifeducation_progress_bar(value = pgr_value,
                                       total = pgr_max_value,
                                       title = "Train Classifier")


      #Checking Minimal Frequencies.
      if(trace==TRUE){
        message(paste(date(),
                    "Checking Minimal Frequencies."))
      }

      freq_check<-table(data_targets)
      freq_check_eval<-freq_check<4
      if(sum(freq_check_eval)>0){
        cat<-subset(names(freq_check),
                    subset = freq_check_eval)
        stop(paste("Categories",cat,"have absolute frequencies below 4.",
             "These categories are not suitable for training.
             Please remove the corresponding categories/classes from the data
             and create a new classifier with the reduced data set."))
      }

      #Checking Number of categories.
      if(length(freq_check)<2){
        stop("After checking for uniquness data only consists of one category.
             At least to categories are neceassry for training.")
      }

      #Split data into k folds
      folds=get_folds(target=data_targets,k_folds=data_n_test_samples)

      #Create a Vector with names of categories
      categories<-names(table(data_targets))

      #Saving Training Information
      if(use_bpl==TRUE){
        n_unlabeled_data=length(data_targets)-length(na.omit(data_targets))
      } else {
        n_unlabeled_data=0
      }
      names(n_unlabeled_data)="unlabeled"
      n_labeled_data=as.vector(table(data_targets))
      names(n_labeled_data)=names(table(data_targets))
      self$last_training$data=append(n_labeled_data,n_unlabeled_data)

      #Init Object for Saving pbl_labeling
      data_pbl=matrix(data=0,
                      nrow=folds$n_folds+1,
                      ncol=bpl_max_steps)
      dimnames(data_pbl)=list(folds=c(paste0("fold_",seq(from=1,to=folds$n_folds,by=1)),
                              "final_train"),
                              bpl_steps=NULL)

      #Init Object for Saving syntehtic xases
      data_bsc_train=matrix(data = 0,
                      nrow = folds$n_folds+1,
                      ncol = length(categories))
      rownames(data_bsc_train)=c(paste0("fold_",seq(from=1,to=folds$n_folds,by=1)),
                                 "final")

      data_bsc_test=data_bsc_train

      #Initializing Objects for Saving Performance
      metric_names=get_coder_metrics(
        true_values=NULL,
        predicted_values=NULL,
        return_names_only=TRUE)

      test_metric=array(dim=c(folds$n_folds,
                             4,
                             length(metric_names)),
                       dimnames = list(iterations=NULL,
                                       steps=c("Baseline",
                                               "BSC",
                                               "BPL",
                                               "Final"),
                                       metrics=metric_names))
      iota_objects_start=NULL
      iota_objects_end=NULL
      iota_objects_start_free=NULL
      iota_objects_end_free=NULL

      #Initializing core objects
      embeddings_all=data_embeddings$embeddings
      targets_labeleld_all=na.omit(data_targets)

      names_unlabeled=names(subset(data_targets,is.na(data_targets)==TRUE))

      #Setting a new ID for the classifier
      private$model_info$model_name=paste0(private$model_info$model_name_root,"_id_",generate_id(16))

      for(iter in 1:folds$n_folds){
        base::gc(verbose = FALSE,full = TRUE)
        #---------------------------------------------
        #Create a Train and Validation Sample
        names_targets_labaled_test=folds$val_sample[[iter]]
        names_targets_labeled_train=folds$train_sample[[iter]]

        targets_labeleld_train=targets_labeleld_all[names_targets_labeled_train]
        targets_labeled_test=targets_labeleld_all[names_targets_labaled_test]

        #Train baseline model or normal training--------------------------------
        if(use_baseline==TRUE |
           (use_bsc==FALSE & use_bpl==FALSE) |
           (use_bsc==FALSE & use_bpl==TRUE)){
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Training Baseline Model"))
          }

          #Get Train and Test Sample
          baseline_sample<-get_stratified_train_test_split(
            targets = targets_labeleld_all[names_targets_labeled_train],
            val_size = bsl_val_size)

          names_targets_labeled_train_train=baseline_sample$train_sample
          names_targets_labeled_train_test=baseline_sample$test_sample

          targets_labeled_train_train=targets_labeleld_all[names_targets_labeled_train_train]
          targets_labeled_val=targets_labeleld_all[names_targets_labeled_train_test]

          #Train model
          tmp_history=private$basic_train(embedding_train=embeddings_all[names_targets_labeled_train_train,,],
                              target_train=targets_labeled_train_train,
                              embedding_val=embeddings_all[names_targets_labeled_train_test,,],
                              target_val=targets_labeled_val,
                              embedding_test = embeddings_all[names_targets_labaled_test,,],
                              target_test = targets_labeled_test,
                              epochs=epochs,
                              batch_size=batch_size,
                              trace=FALSE,
                              keras_trace=keras_trace,
                              pytorch_trace=pytorch_trace,
                              reset_model=TRUE,
                              dir_checkpoint=dir_checkpoint,
                              balance_class_weights=balance_class_weights,
                              shiny_app_active=shiny_app_active)
          self$last_training$history$bsl[iter]=list(tmp_history)

          #Predict val targets
          test_predictions=self$predict(newdata = embeddings_all[names_targets_labaled_test,,],
                                        verbose = keras_trace,
                                        batch_size =batch_size)
          test_pred_cat=test_predictions$expected_category
          names(test_pred_cat)=rownames(test_predictions)
          test_pred_cat<-test_pred_cat[names(targets_labeled_test)]
          test_res=get_coder_metrics(true_values = targets_labeled_test,
                                     predicted_values = test_pred_cat)

          #Save results for baseline model
          test_metric[iter,1,]<-test_res
          #message(paste("Baseline:",test_res["avg_alpha"]))

          if(use_bsc==TRUE | use_bpl==TRUE){
          iota_objects_start[iter]=list(iotarelr::check_new_rater(true_values = targets_labeled_test,
                                                                  assigned_values = test_pred_cat,
                                                                  free_aem = FALSE))
          iota_objects_start_free[iter]=list(iotarelr::check_new_rater(true_values = targets_labeled_test,
                                                                       assigned_values = test_pred_cat,
                                                                       free_aem = TRUE))
          }
          #message(paste("Train",length(targets_labeled_train_train),
          #          "Validation",length(targets_labeled_val),
          #          "Test",length(targets_labeled_test)))
          pgr_value=pgr_value+1
          update_aifeducation_progress_bar(value = pgr_value,
                                           total = pgr_max_value,
                                           title = "Train Classifier")
        }

        #Create and Train with synthetic cases----------------------------------
        if(use_bsc==TRUE){
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Applying Augmention with Balanced Synthetic Cases"))
          }

          #Generating Data For Training
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Generating Synthetic Cases"))
          }
          #save(embeddings_train_labeled,targets_train_labeled,bsc_methods,bsc_max_k,
          #     file="debug.RData")

          syn_cases<-get_synthetic_cases(embedding=embeddings_all[names_targets_labeled_train,,],
                                         target=targets_labeleld_train,
                                         method=bsc_methods,
                                         max_k=bsc_max_k,
                                         times = private$text_embedding_model$times,
                                         features = private$text_embedding_model$features)
          targets_synthetic_all=factor(syn_cases$syntetic_targets,
                                       levels = categories)
          embeddings_syntehtic_all=syn_cases$syntetic_embeddings

          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Generating Synthetic Cases Done"))
          }

          #Combining original labeled data and synthetic data
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Selecting Synthetic Cases"))
          }
          #Checking frequencies of categories and adding syn_cases
          cat_freq=table(targets_labeleld_train)
          cat_max=max(cat_freq)
          cat_delta=cat_max-cat_freq

          if(bsc_add_all==TRUE){
            cat_delta[]=Inf
          }

          cat_freq_syn=table(targets_synthetic_all)

          names_syntethic_targets_selected=NULL
          for(cat in categories){
            if(cat_delta[cat]>0){
              condition=(targets_synthetic_all==cat)
              tmp_subset=subset(x = targets_synthetic_all,
                                subset = condition)
              names_syntethic_targets_selected[cat]=list(
                sample(x=names(tmp_subset),
                       size = min(cat_delta[cat],length(tmp_subset)),
                       replace = FALSE)
              )
            }
          }
          names_syntethic_targets_selected=(unlist(names_syntethic_targets_selected,
                                                   use.names = FALSE))

          #Combining original labeled data and synthetic data
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Combining Original and Synthetic Data"))
          }

          #embeddings_labeled_syn=rbind(embeddings,
          #                             syn_cases$syntetic_embeddings[syn_cases_selected,])
          #targets_labeled_syn=c(targets_train_labeled,as.factor(syn_cases$syntetic_targets[syn_cases_selected]))
          #targets_labeled_syn=targets_labeled_syn[rownames(embeddings_labeled_syn)]

          #Creating training and test sample
          bsc_train_test_split<-get_stratified_train_test_split(
            targets = c(targets_labeleld_train,targets_synthetic_all[names_syntethic_targets_selected]),
            val_size=bsc_val_size)

          #Including names of synthetic cases
          names_targets_labeled_train_train=bsc_train_test_split$train_sample
          names_targets_labeled_train_test=bsc_train_test_split$test_sample

          #Creating the final dataset for training. Please note that units
          #with NA in target are included for pseudo labeling if requested
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Creating Train Dataset"))
          }

          embeddings_all_and_synthetic=abind::abind(
            embeddings_all,
            embeddings_syntehtic_all,
            along = 1)

          targets_all_and_synthetic=c(
            targets_labeleld_all,
            targets_synthetic_all)

          #Creating the final test dataset for training
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Creating Test Dataset"))
          }

          #Save freq of every labeled original and synthetic case
          data_bsc_train[iter,]<-table(targets_all_and_synthetic[names_targets_labeled_train_train])
          data_bsc_test[iter,]<-table(targets_all_and_synthetic[names_targets_labeled_train_test])

          #Train model
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Start Training"))
          }
          tmp_history=private$basic_train(embedding_train=embeddings_all_and_synthetic[names_targets_labeled_train_train,,],
                              target_train=targets_all_and_synthetic[names_targets_labeled_train_train],
                              embedding_val=embeddings_all_and_synthetic[names_targets_labeled_train_test,,],
                              target_val=targets_all_and_synthetic[names_targets_labeled_train_test],
                              embedding_test = embeddings_all[names_targets_labaled_test,,],
                              target_test = targets_labeled_test,
                              epochs=epochs,
                              batch_size=batch_size,
                              trace=FALSE,
                              keras_trace=keras_trace,
                              pytorch_trace=pytorch_trace,
                              reset_model=TRUE,
                              dir_checkpoint=dir_checkpoint,
                              balance_class_weights=balance_class_weights,
                              shiny_app_active=shiny_app_active)
          self$last_training$history$bsc[iter]=list(tmp_history)

          #Predict val targets
          test_predictions=self$predict(newdata = embeddings_all[names_targets_labaled_test,,],
                                        verbose = keras_trace,
                                        batch_size =batch_size)
          test_pred_cat=test_predictions$expected_category
          names(test_pred_cat)=rownames(test_predictions)
          test_pred_cat=test_pred_cat[names(targets_labeled_test)]
          test_res=get_coder_metrics(true_values = targets_labeled_test,
                                     predicted_values = test_pred_cat)
          #Save results for baseline model
          test_metric[iter,2,]<-test_res
          #message(paste("BSC",test_res["avg_alpha"]))

          #message(paste("Train",length(names_targets_labeled_train_train),
          #            "Validation",length(names_targets_labeled_train_test),
          #            "Test",length(targets_labeled_test)))
          pgr_value=pgr_value+1
          update_aifeducation_progress_bar(value = pgr_value,
                                           total = pgr_max_value,
                                           title = "Train Classifier")

        }
        base::gc(verbose = FALSE,full = TRUE)
        #Applying Pseudo Labeling-----------------------------------------------
        if(use_bpl==TRUE){
          categories<-names(table(data_targets))
          if(trace==TRUE){
            message(paste(date(),
                        "Iter:",iter,"from",folds$n_folds,
                        "Applying Pseudo Labeling"))
          }

          #Defining the basic parameter for while
          step=1
          best_step_balanced_accuracy=-100

          pseudo_label_targets_labeled_test=targets_labeled_test

          if(use_bsc==TRUE){
            pseudo_label_embeddings_all=embeddings_all_and_synthetic
            pseudo_label_targets_labeled_train=targets_all_and_synthetic[names_targets_labeled_train_train]
            pseudo_label_targets_labeled_val=targets_all_and_synthetic[names_targets_labeled_train_test]
          } else {
            pseudo_label_embeddings_all=embeddings_all
            pseudo_label_targets_labeled_train=targets_labeled_train_train
            pseudo_label_targets_labeled_val=targets_labeled_val
          }
          weights_cases_list=NULL
          weights_cases_list[1]=list(names_targets_labeled_train_train)

          added_cases_train=100


          #Start of While-------------------------------------------------------
          step_histories<-NULL
          while(step <=bpl_max_steps & added_cases_train>0){
            base::gc(verbose = FALSE,full = TRUE)
            if(bpl_dynamic_inc==TRUE){
              bpl_inc_ratio=step/bpl_max_steps
            } else {
              bpl_inc_ratio=1
            }

            #message(paste("Train",length(pseudo_label_targets_labeled_train),
            #            "Validation",length(pseudo_label_targets_labeled_val),
            #            "Test",length(pseudo_label_targets_labeled_test),
            #            "Unlabeled",length(names_unlabeled)))


            #Estimate the labels for the remaining data
            est_remaining_data=self$predict(newdata=embeddings_all[names_unlabeled,,],
                                            verbose = keras_trace,
                                            batch_size =batch_size)

            #Create Matrix for saving the results
            new_categories<-matrix(nrow= nrow(est_remaining_data),
                                   ncol=2)
            rownames(new_categories)=rownames(est_remaining_data)
            colnames(new_categories)=c("cat","prob")

            #Gather information for every case. That is the category with the
            #highest probability and save both
            for(i in 1:nrow(est_remaining_data)){
              tmp_est_prob=est_remaining_data[i,1:(ncol(est_remaining_data)-1)]
              new_categories[i,1]=categories[which.max(tmp_est_prob)]
              new_categories[i,2]=max(tmp_est_prob)
            }
            new_categories<-as.data.frame(new_categories)

            #Transforming the probabilities to a information index
            new_categories[,2]=abs(bpl_anchor-(as.numeric(new_categories[,2])-1/length(categories))/(1-1/length(categories)))
            new_categories=as.data.frame(new_categories)

            #Reducing the new categories to the desired range
            condition=(new_categories[,2]>=bpl_min & new_categories[,2]<=bpl_max)
            new_categories=subset(new_categories,
                                  condition)

            #calculate the minimal freq of the available categories
            new_cat_freq=table(new_categories$cat)
            min_new_freq=max(floor(min(new_cat_freq)*bpl_inc_ratio),1)
            new_categories_names=names(new_cat_freq)

            #Order cases with increasing distance from maximal information
            names_final_new_categories=NULL
            for(cat in new_categories_names){
              condition=(new_categories[,1]==cat)
              tmp=subset(x=new_categories,
                         subset=condition)
              tmp=tmp[order(tmp$prob,decreasing = FALSE),]
              if(bpl_balance==TRUE){
                #Chose always the same number of new cases to ensure the balance
                #of all categories
                names_final_new_categories=append(x=names_final_new_categories,
                                            values=rownames(tmp)[1:min_new_freq])
              } else {
                n_inc=max(floor(nrow(tmp)*bpl_inc_ratio),1)
                names_final_new_categories=append(x=names_final_new_categories,
                                            values=rownames(tmp)[1:n_inc])
              }
            }

            new_categories<-new_categories[names_final_new_categories,]

            targets_pseudo_labeled<-new_categories[names_final_new_categories,1]
            targets_pseudo_labeled<-factor(targets_pseudo_labeled,
                                         levels=categories)
            names(targets_pseudo_labeled)<-names_final_new_categories

            targets_labeled_and_pseudo<-c(
              pseudo_label_targets_labeled_train,
              targets_pseudo_labeled)

          #Counting new cases
          added_cases_train=length(targets_pseudo_labeled)
          data_pbl[iter,step]=added_cases_train

          #Calculating the weights for the new cases
          weights_cases_list[2]=list(names(targets_pseudo_labeled))
          tmp_weights=NULL
          tmp_weights_names=NULL
          for(i in 1:length(weights_cases_list)){
            if(i==1){
              w=1
            } else {
              w=bpl_weight_start+ bpl_weight_inc*step
            }
            tmp_weights=append(x=tmp_weights,
                                 values = rep(
                                   x=w,
                                   times = length(weights_cases_list[[i]]))
                               )
            tmp_weights_names=append(x=tmp_weights_names,
                                     values = weights_cases_list[[i]])
            }
            names(tmp_weights)=tmp_weights_names
            sample_weights=tmp_weights[names(targets_labeled_and_pseudo)]

           #Train model
            if(bpl_epochs_per_step>1){
              use_callback=TRUE
            } else {
              use_callback=FALSE
            }
            tmp_history=private$basic_train(embedding_train=pseudo_label_embeddings_all[names(targets_labeled_and_pseudo),,],
                                    target_train=targets_labeled_and_pseudo,
                                    embedding_val=pseudo_label_embeddings_all[names(pseudo_label_targets_labeled_val),,],
                                    target_val=pseudo_label_targets_labeled_val,
                                    embedding_test = embeddings_all[names_targets_labaled_test,,],
                                    target_test = targets_labeled_test,
                                    epochs = bpl_epochs_per_step,
                                    use_callback=use_callback,
                                    batch_size=batch_size,
                                    trace=FALSE,
                                    keras_trace=keras_trace,
                                    pytorch_trace=pytorch_trace,
                                    reset_model=bpl_model_reset,
                                    dir_checkpoint=dir_checkpoint,
                                    sample_weights=sample_weights,
                                    balance_class_weights=balance_class_weights,
                                    shiny_app_active=shiny_app_active)
            #self$last_training$history$bpl[iter]=list(NULL)
            step_histories[step]=list(tmp_history)

              #predict val targets
              val_predictions=self$predict(newdata=pseudo_label_embeddings_all[names(pseudo_label_targets_labeled_val),,],
                                            verbose = keras_trace,
                                            batch_size =batch_size)
              val_pred_cat=val_predictions$expected_category
              names(val_pred_cat)=rownames(val_predictions)
              val_pred_cat=val_pred_cat[names(pseudo_label_targets_labeled_val)]
              val_res=get_coder_metrics(true_values = pseudo_label_targets_labeled_val,
                                        predicted_values = val_pred_cat)

              #Predict test targets
              test_predictions=self$predict(newdata = embeddings_all[names(pseudo_label_targets_labeled_test),,],
                                            verbose = keras_trace,
                                            batch_size =batch_size)
              test_pred_cat=test_predictions$expected_category
              names(test_pred_cat)=rownames(test_predictions)
              test_pred_cat<-test_pred_cat[names(pseudo_label_targets_labeled_test)]
              test_res=get_coder_metrics(true_values = pseudo_label_targets_labeled_test,
                                         predicted_values = test_pred_cat)

              if(best_step_balanced_accuracy<val_res["balanced_accuracy"]){
                best_step_balanced_accuracy=val_res["balanced_accuracy"]

                if(private$ml_framework=="tensorflow"){
                  self$model$save_weights(paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.h5"))
                } else if(private$ml_framework=="pytorch"){
                  torch$save(self$model$state_dict(),paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.pt"))
                }

                best_val_metric=val_res
                best_test_metric=test_res
              }

              if(trace==TRUE){
                message(paste(date(),
                            "Step",step,"Done"))
              }

            pgr_value=pgr_value+1
            update_aifeducation_progress_bar(value = pgr_value,
                                             total = pgr_max_value,
                                             title = "Train Classifier")

            #increase step
            step=step+1
          }
          self$last_training$history$bpl[iter]=list(step_histories)

          #self$model=keras$models$clone_model(new_best_model)
          if(private$ml_framework=="tensorflow"){
            self$model$load_weights(paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.h5"))
          } else if(private$ml_framework=="pytorch"){
            self$model$load_state_dict(torch$load(paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.pt")))
          }

          test_metric[iter,"BPL",]<-best_test_metric
          test_res<-best_test_metric
        }
        test_metric[iter,"Final",]<-test_res


        test_predictions=self$predict(newdata = embeddings_all[names_targets_labaled_test,,],
                                      verbose = keras_trace,
                                      batch_size =batch_size)
        test_pred_cat=test_predictions$expected_category
        names(test_pred_cat)=rownames(test_predictions)
        test_pred_cat<-test_pred_cat[names(targets_labeled_test)]
        test_res=get_coder_metrics(true_values = targets_labeled_test,
                                   predicted_values = test_pred_cat)
        iota_objects_end[iter]=list(iotarelr::check_new_rater(true_values = targets_labeled_test,
                                                                assigned_values = test_pred_cat,
                                                                free_aem = FALSE))
        iota_objects_end_free[iter]=list(iotarelr::check_new_rater(true_values = targets_labeled_test,
                                                                     assigned_values = test_pred_cat,
                                                                     free_aem = TRUE))
        self$reliability$standard_measures_end[iter]=list(calc_standard_classification_measures(true_values=targets_labeled_test,
                                                                               predicted_values=test_pred_cat))

        #----------------------------------------------
      }

      #Insert Final Training here and Savings here
      self$reliability$test_metric=test_metric
      self$reliability$raw_iota_objects$iota_objects_start=iota_objects_start
      self$reliability$raw_iota_objects$iota_objects_end=iota_objects_end
      self$reliability$raw_iota_objects$iota_objects_start_free=iota_objects_start_free
      self$reliability$raw_iota_objects$iota_objects_end_free=iota_objects_end_free

      if(is.null(iota_objects_start)==FALSE){
        self$reliability$iota_object_start=create_iota2_mean_object(
          iota2_list = iota_objects_start,
          original_cat_labels = categories,
          free_aem=FALSE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_object_start=NULL
      }

      if(is.null(iota_objects_end)==FALSE){
        self$reliability$iota_object_end=create_iota2_mean_object(
          iota2_list = iota_objects_end,
          original_cat_labels = categories,
          free_aem=FALSE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_end=NULL
      }

      if(is.null(iota_objects_start_free)==FALSE){
        self$reliability$iota_object_start_free=create_iota2_mean_object(
          iota2_list = iota_objects_start_free,
          original_cat_labels = categories,
          free_aem=TRUE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_start_free=NULL
      }

      if(is.null(iota_objects_end_free)==FALSE){
        self$reliability$iota_object_end_free=create_iota2_mean_object(
          iota2_list = iota_objects_end_free,
          original_cat_labels = categories,
          free_aem=TRUE,
          call="aifeducation::te_classifier_neuralnet")
      } else {
        self$reliability$iota_objects_end_free=NULL
      }


      standard_measures_mean_table<-matrix(
        nrow = length(self$model_config$target_levels),
        ncol = 3,
        data = 0)
      colnames(standard_measures_mean_table)=c("precision","recall","f1")
      rownames(standard_measures_mean_table)<-self$model_config$target_levels

      for(i in 1:folds$n_folds){
        for(tmp_cat in self$model_config$target_levels){
          standard_measures_mean_table[tmp_cat,"precision"]=standard_measures_mean_table[tmp_cat,"precision"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"precision"]
          standard_measures_mean_table[tmp_cat,"recall"]=standard_measures_mean_table[tmp_cat,"recall"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"recall"]
          standard_measures_mean_table[tmp_cat,"f1"]=standard_measures_mean_table[tmp_cat,"f1"]+
            self$reliability$standard_measures_end[[i]][tmp_cat,"f1"]
        }
      }
      standard_measures_mean_table=standard_measures_mean_table/folds$n_folds

      self$reliability$standard_measures_mean<-standard_measures_mean_table

      #Final Training----------------------------------------------------------
      base::gc(verbose = FALSE,full = TRUE)
      if((use_bsc==FALSE & use_bpl==FALSE) |
         #(use_baseline=TRUE) |
        (use_bsc==FALSE & use_bpl==TRUE)){
        embeddings_train=data_embeddings$clone(deep=TRUE)
        targets_train=data_targets
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training of Baseline Model"))
        }
        #Get Train and Test Sample
        baseline_sample<-get_stratified_train_test_split(
          targets = targets_labeleld_all,
          val_size = bsl_val_size)

        names_targets_labeled_train_train=baseline_sample$train_sample
        names_targets_labeled_train_test=baseline_sample$test_sample

        targets_labeled_train_train=targets_labeleld_all[names_targets_labeled_train_train]
        targets_labeled_val=targets_labeleld_all[names_targets_labeled_train_test]

        #Train model
        tmp_history=private$basic_train(embedding_train=embeddings_all[names_targets_labeled_train_train,,],
                            target_train=targets_labeled_train_train,
                            embedding_val=embeddings_all[names_targets_labeled_train_test,,],
                            target_val=targets_labeled_val,
                            embedding_test = NULL,
                            target_test = NULL,
                            epochs=epochs,
                            batch_size=batch_size,
                            trace=FALSE,
                            keras_trace=keras_trace,
                            pytorch_trace=pytorch_trace,
                            reset_model=TRUE,
                            dir_checkpoint=dir_checkpoint,
                            balance_class_weights=balance_class_weights,
                            shiny_app_active=shiny_app_active)
        self$last_training$history$bsl["final"]=list(tmp_history)

        pgr_value=pgr_value+1
        update_aifeducation_progress_bar(value = pgr_value,
                                         total = pgr_max_value,
                                         title = "Train Classifier")


        #message(paste("Train",length(names_targets_labeled_train_train),
        #            "Validation",length(names_targets_labeled_train_test)))

      }

      #Final Training with BSC--------------------------------------------------
      base::gc(verbose = FALSE,full = TRUE)
      if(use_bsc==TRUE){
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Applying Augmention with Balanced Synthetic Cases"))
        }
        #Generating Data For Training
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Generating Synthetic Cases"))
        }
        #save(embeddings_train_labeled,targets_train_labeled,bsc_methods,bsc_max_k,
        #     file="debug.RData")
        syn_cases<-get_synthetic_cases(embedding=embeddings_all[names(targets_labeleld_all),,],
                                       target=targets_labeleld_all,
                                       method=bsc_methods,
                                       max_k=bsc_max_k,
                                       times = private$text_embedding_model$times,
                                       features = private$text_embedding_model$features)
        targets_synthetic_all=factor(syn_cases$syntetic_targets,
                                     levels = categories)
        embeddings_syntehtic_all=syn_cases$syntetic_embeddings

        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Generating Synthetic Cases Done"))
        }

        #Combining original labeled data and synthetic data
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Selecting Synthetic Cases"))
        }
        #Checking frequencies of categories and adding syn_cases
        cat_freq=table(targets_labeleld_all)
        cat_max=max(cat_freq)
        cat_delta=cat_max-cat_freq

        if(bsc_add_all==TRUE){
          cat_delta[]=Inf
        }

        cat_freq_syn=table(targets_synthetic_all)

        names_syntethic_targets_selected=NULL
        for(cat in categories){
          if(cat_delta[cat]>0){
            condition=(targets_synthetic_all==cat)
            tmp_subset=subset(x = targets_synthetic_all,
                              subset = condition)
            names_syntethic_targets_selected[cat]=list(
              sample(x=names(tmp_subset),
                     size = min(cat_delta[cat],length(tmp_subset)),
                     replace = FALSE)
            )
          }
        }
        names_syntethic_targets_selected=(unlist(names_syntethic_targets_selected,
                                                 use.names = FALSE))

        #Combining original labeled data and synthetic data
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Combining Original and Synthetic Data"))
        }

        #Creating training and test sample
        bsc_train_test_split<-get_stratified_train_test_split(
          targets = c(targets_labeleld_all,targets_synthetic_all[names_syntethic_targets_selected]),
          val_size=bsc_val_size)

        #Including names of synthetic cases
        names_targets_labeled_train_train=bsc_train_test_split$train_sample
        names_targets_labeled_train_test=bsc_train_test_split$test_sample

        #Creating the final dataset for training. Please note that units
        #with NA in target are included for pseudo labeling if requested
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Creating Train Dataset"))
        }

        embeddings_all_and_synthetic=abind::abind(
          embeddings_all,
          embeddings_syntehtic_all,
          along = 1)

        targets_all_and_synthetic=c(
          targets_labeleld_all,
          targets_synthetic_all)

        #Creating the final test dataset for training
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Creating Test Dataset"))
        }

        #Save freq of every labeled original and synthetic case
        data_bsc_train["final",]<-table(targets_all_and_synthetic[names_targets_labeled_train_train])
        data_bsc_test["final",]<-table(targets_all_and_synthetic[names_targets_labeled_train_test])

        #Train model
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Start Training"))
        }
        tmp_history=private$basic_train(embedding_train=embeddings_all_and_synthetic[names_targets_labeled_train_train,,],
                            target_train=targets_all_and_synthetic[names_targets_labeled_train_train],
                            embedding_val=embeddings_all_and_synthetic[names_targets_labeled_train_test,,],
                            target_val=targets_all_and_synthetic[names_targets_labeled_train_test],
                            embedding_test = NULL,
                            target_test = NULL,
                            epochs=epochs,
                            batch_size=batch_size,
                            trace=FALSE,
                            keras_trace=keras_trace,
                            pytorch_trace=pytorch_trace,
                            reset_model=TRUE,
                            dir_checkpoint=dir_checkpoint,
                            balance_class_weights=balance_class_weights,
                            shiny_app_active=shiny_app_active)
        self$last_training$history$bsc["final"]=list(tmp_history)
        #message(paste("Train",length(names_targets_labeled_train_train),
        #            "Validation",length(names_targets_labeled_train_test)))

        pgr_value=pgr_value+1
        update_aifeducation_progress_bar(value = pgr_value,
                                         total = pgr_max_value,
                                         title = "Train Classifier")
      }

        #Applying Pseudo Labeling-----------------------------------------------
      if(use_bpl==TRUE){
        categories<-names(table(data_targets))
        if(trace==TRUE){
          message(paste(date(),
                      "Final Training",
                      "Applying Pseudo Labeling"))
        }

        #Defining the basic parameter for while
        step=1
        best_step_balanced_accuracy=-100

        if(use_bsc==TRUE){
          pseudo_label_embeddings_all=embeddings_all_and_synthetic
          pseudo_label_targets_labeled_train=targets_all_and_synthetic[names_targets_labeled_train_train]
          pseudo_label_targets_labeled_test=targets_labeled_test
          pseudo_label_targets_labeled_val=targets_all_and_synthetic[names_targets_labeled_train_test]
        } else {
          pseudo_label_embeddings_all=embeddings_all
          pseudo_label_targets_labeled_train=targets_labeled_train_train
          pseudo_label_targets_labeled_val=targets_labeled_val
          pseudo_label_targets_labeled_test=targets_labeled_test
        }
        weights_cases_list=NULL
        weights_cases_list[1]=list(names_targets_labeled_train_train)

        added_cases_train=100


        #Start of While-------------------------------------------------------
        step_histories<-NULL
        while(step <=bpl_max_steps & added_cases_train>0){
          base::gc(verbose = FALSE,full = TRUE)
          #message(paste("Train",length(pseudo_label_targets_labeled_train),
          #            "Validation",length(pseudo_label_targets_labeled_val),
          #            "Unlabeled",length(names_unlabeled)))

          if(bpl_dynamic_inc==TRUE){
            bpl_inc_ratio=step/bpl_max_steps
          } else {
            bpl_inc_ratio=1
          }

          #Estimate the labels for the remaining data
          est_remaining_data=self$predict(newdata=embeddings_all[names_unlabeled,,],
                                          verbose = keras_trace,
                                          batch_size =batch_size)

          #Create Matrix for saving the results
          new_categories<-matrix(nrow= nrow(est_remaining_data),
                                 ncol=2)
          rownames(new_categories)=rownames(est_remaining_data)
          colnames(new_categories)=c("cat","prob")

          #Gather information for every case. That is the category with the
          #highest probability and save both
          for(i in 1:nrow(est_remaining_data)){
            tmp_est_prob=est_remaining_data[i,1:(ncol(est_remaining_data)-1)]
            new_categories[i,1]=categories[which.max(tmp_est_prob)]
            new_categories[i,2]=max(tmp_est_prob)
          }
          new_categories<-as.data.frame(new_categories)

          #Transforming the probabilities to a information index
          new_categories[,2]=abs(bpl_anchor-(as.numeric(new_categories[,2])-1/length(categories))/(1-1/length(categories)))
          new_categories=as.data.frame(new_categories)

          #Reducing the new categories to the desired range
          condition=(new_categories[,2]>=bpl_min & new_categories[,2]<=bpl_max)
          new_categories=subset(new_categories,
                                condition)

          #calculate the minimal freq of the available categories
          new_cat_freq=table(new_categories$cat)
          min_new_freq=max(floor(min(new_cat_freq)*bpl_inc_ratio),1)
          new_categories_names=names(new_cat_freq)

          #Order cases with increasing distance from maximal information
          names_final_new_categories=NULL
          for(cat in new_categories_names){
            condition=(new_categories[,1]==cat)
            tmp=subset(x=new_categories,
                       subset=condition)
            tmp=tmp[order(tmp$prob,decreasing = FALSE),]
            if(bpl_balance==TRUE){
              #Chose always the same number of new cases to ensure the balance
              #of all categories
              names_final_new_categories=append(x=names_final_new_categories,
                                                values=rownames(tmp)[1:min_new_freq])
            } else {
              n_inc=max(floor(nrow(tmp)*bpl_inc_ratio),1)
              names_final_new_categories=append(x=names_final_new_categories,
                                                values=rownames(tmp)[1:n_inc])
            }
          }

          new_categories<-new_categories[names_final_new_categories,]


          targets_pseudo_labeled<-new_categories[names_final_new_categories,1]
          targets_pseudo_labeled<-factor(targets_pseudo_labeled,
                                         levels=categories)
          names(targets_pseudo_labeled)<-names_final_new_categories

          targets_labeled_and_pseudo<-c(
            pseudo_label_targets_labeled_train,
            targets_pseudo_labeled)

          #Counting new cases
          added_cases_train=length(targets_pseudo_labeled)
          data_pbl["final_train",step]=added_cases_train

          #Calculating the weights for the new cases
          weights_cases_list[2]=list(names(targets_pseudo_labeled))
          tmp_weights=NULL
          tmp_weights_names=NULL
          for(i in 1:length(weights_cases_list)){
            if(i==1){
              w=1
            } else {
              bpl_weight_start+ bpl_weight_inc*step
            }
            tmp_weights=append(x=tmp_weights,
                               values = rep(
                                 x=w,
                                 times = length(weights_cases_list[[i]]))
            )
            tmp_weights_names=append(x=tmp_weights_names,
                                     values = weights_cases_list[[i]])
          }
          names(tmp_weights)=tmp_weights_names
          sample_weights=tmp_weights[names(targets_labeled_and_pseudo)]

          #Train model
          if(bpl_epochs_per_step>1){
            use_callback=TRUE
          } else {
            use_callback=FALSE
          }
          tmp_history=private$basic_train(embedding_train=pseudo_label_embeddings_all[names(targets_labeled_and_pseudo),,],
                              target_train=targets_labeled_and_pseudo,
                              embedding_val=pseudo_label_embeddings_all[names(pseudo_label_targets_labeled_val),,],
                              target_val=pseudo_label_targets_labeled_val,
                              embedding_test = NULL,
                              target_test = NULL,
                              epochs = bpl_epochs_per_step,
                              use_callback=use_callback,
                              batch_size=batch_size,
                              trace=FALSE,
                              keras_trace=keras_trace,
                              pytorch_trace=pytorch_trace,
                              reset_model=bpl_model_reset,
                              dir_checkpoint=dir_checkpoint,
                              sample_weights=sample_weights,
                              balance_class_weights=balance_class_weights,
                              shiny_app_active=shiny_app_active)
          #self$last_training$history$bpl["final"]=list(NULL)
          step_histories[step]=list(tmp_history)

          #message(paste("Train",length(targets_labeled_and_pseudo),
          #            "Validation",length(pseudo_label_targets_labeled_val),
          #            "Unlabeled",length(targets_pseudo_labeled)))

          #predict val targets
          val_predictions=self$predict(newdata=pseudo_label_embeddings_all[names(pseudo_label_targets_labeled_val),,],
                                       verbose = keras_trace,
                                       batch_size =batch_size)
          val_pred_cat=val_predictions$expected_category
          names(val_pred_cat)=rownames(val_predictions)
          val_pred_cat=val_pred_cat[names(pseudo_label_targets_labeled_val)]
          val_res=get_coder_metrics(true_values = pseudo_label_targets_labeled_val,
                                    predicted_values = val_pred_cat)

          if(best_step_balanced_accuracy<val_res["balanced_accuracy"]){
            best_step_balanced_accuracy=val_res["balanced_accuracy"]

            if(private$ml_framework=="tensorflow"){
              self$model$save_weights(paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.h5"))
            } else if(private$ml_framework=="pytorch"){
              torch$save(self$model$state_dict(),paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.pt"))
            }
            #self$model$save_weights(paste0(dir_checkpoint,"/bpl_best_weights.h5"))
            best_val_metric=val_res
          }
          #message(paste("Validation:",val_res["avg_alpha"]))

          if(trace==TRUE){
            message(paste(date(),
                        "Step",step,"Done"))
          }

          pgr_value=pgr_value+1
          update_aifeducation_progress_bar(value = pgr_value,
                                           total = pgr_max_value,
                                           title = "Train Classifier")

          #increase step
          step=step+1
        }
        self$last_training$history$bpl["final"]=list(step_histories)

        #self$model=keras$models$clone_model(new_best_model)
        if(private$ml_framework=="tensorflow"){
          self$model$load_weights(paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.h5"))
        } else if(private$ml_framework=="pytorch"){
          self$model$load_state_dict(torch$load(paste0(dir_checkpoint,"/checkpoints/bpl_best_weights.pt")))
        }
      }
      #Save Final Information
      self$last_training$date=date()
      self$last_training$config$use_bsc=use_bsc
      self$last_training$config$use_baseline=use_baseline
      self$last_training$config$use_bpl=use_bpl
      self$last_training$n_samples=folds$n_folds

      self$last_training$data_bsc_train=data_bsc_train
      self$last_training$data_bsc_test=data_bsc_test

      if(use_bpl==TRUE){
        self$last_training$data_pbl=data_pbl
      } else {
        self$last_training$data_pbl=NULL
      }

      test_metric_mean=matrix(data=0,
                             nrow = nrow(test_metric[1,,]),
                             ncol = ncol(test_metric[1,,]))
      rownames(test_metric_mean)=rownames(test_metric[1,,])
      colnames(test_metric_mean)=colnames(test_metric[1,,])

      n_mean=vector(length = nrow(test_metric[1,,]))
      n_mean[]=folds$n_folds

      for(i in 1:folds$n_folds){
          tmp_val_metric=test_metric[i,,]
          for(j in 1:nrow(tmp_val_metric)){
            if(sum(is.na(tmp_val_metric[j,]))!=length(tmp_val_metric[j,])){
              test_metric_mean[j,]=test_metric_mean[j,]+tmp_val_metric[j,]
            } else {
              n_mean[j]=n_mean[j]-1
            }
          }
        }

      test_metric_mean=test_metric_mean/n_mean
      test_metric_mean[is.nan(test_metric_mean)]=NA
      self$reliability$test_metric_mean=test_metric_mean

      self$last_training$learning_time=as.numeric(difftime(Sys.time(),start_time,
                                                           units="mins"))

      #Unload Cluster for Parallel Execution
      if(use_bsc==TRUE){
        parallel::stopCluster(cl)
      }

      if(sustain_track==TRUE){
        sustainability_tracker$stop()
        private$sustainability<-summarize_tracked_sustainability(sustainability_tracker)
      } else {
        private$sustainability=list(
          sustainability_tracked=FALSE,
          date=NA,
          sustainability_data=list(
            duration_sec=NA,
            co2eq_kg=NA,
            cpu_energy_kwh=NA,
            gpu_energy_kwh=NA,
            ram_energy_kwh=NA,
            total_energy_kwh=NA
          )
        )
      }

      if(trace==TRUE){
        message(paste(date(),
                    "Training Complete"))
      }
    },
    #-------------------------------------------------------------------------
    #'@description Method for predicting new data with a trained neural net.
    #'@param newdata Object of class \code{TextEmbeddingModel} or
    #'\code{data.frame} for which predictions should be made.
    #'@param verbose \code{int} \code{verbose=0} does not cat any
    #'information about the training process from keras on the console.
    #'\code{verbose=1} prints a progress bar. \code{verbose=2} prints
    #'one line of information for every epoch.
    #'@param batch_size \code{int} Size of batches.
    #'@return Returns a \code{data.frame} containing the predictions and
    #'the probabilities of the different labels for each case.
    predict=function(newdata,
                     batch_size=32,
                     verbose=1){

      #Load Custom Model Scripts
      if(private$ml_framework=="tensorflow"){
        reticulate::py_run_file(system.file("python/keras_te_classifier.py",
                                            package = "aifeducation"))
      } else if(private$ml_framework=="pytorch"){
        reticulate::py_run_file(system.file("python/pytorch_te_classifier.py",
                                            package = "aifeducation"))
      }

      #Checking input data
      #if(methods::isClass(where=newdata,"data.frame")==FALSE){
      #  stop("newdata mus be a data frame")
      #}
      if("EmbeddedText" %in% class(newdata)){
        if(self$check_embedding_model(newdata)==FALSE){
          stop("The TextEmbeddingModel that generated the newdata is not
               the same as the TextEmbeddingModel when generating the classifier.")
        }
        real_newdata=newdata$embeddings
      } else {
        real_newdata=newdata
      }

      #Ensuring the correct order of the variables for prediction
      real_newdata<-real_newdata[,,self$model_config$input_variables,drop=FALSE]
      current_row_names=rownames(real_newdata)

      if(is.null(self$model_config$rec)==TRUE){
        n_rec=0
      } else {
        n_rec=length(self$model_config$rec)
      }

      if(n_rec==0 & self$model_config$repeat_encoder==0){
        real_newdata=array_to_matrix(real_newdata)
      }

      model<-self$model

      if(length(self$model_config$target_levels)>2){
        #Multi Class
        if(private$ml_framework=="tensorflow"){
          predictions_prob<-model$predict(
            x = np$array(real_newdata),
            batch_size = as.integer(batch_size),
            verbose=as.integer(verbose))
        } else if(private$ml_framework=="pytorch"){
          pytorch_predict_data=torch$utils$data$TensorDataset(
            torch$from_numpy(np$array(real_newdata)))
          #predictloader=torch$utils$data$DataLoader(
          #  pytorch_predict_data,
          #  batch_size=as.integer(batch_size),
          #  shuffle=FALSE)

          if(torch$cuda$is_available()){
            device="cuda"
            dtype=torch$double
            model$to(device,dtype=dtype)
            model$eval()
            input=torch$from_numpy(np$array(real_newdata))
            predictions_prob<-model(input$to(device,dtype=dtype),
                                    predication_mode=TRUE)$detach()$cpu()$numpy()
          } else {
            device="cpu"
            dtype=torch$float
            model$to(device,dtype=dtype)
            model$eval()
            input=torch$from_numpy(np$array(real_newdata))
            predictions_prob<-model(input$to(device,dtype=dtype),
                                    predication_mode=TRUE)$detach()$numpy()
          }
        }


        #Select index of column with maximum value and convert to zero based indices
        predictions<-max.col(predictions_prob)-1
      } else {
        if(private$ml_framework=="tensorflow"){
          predictions_prob<-model$predict(
            x = np$array(real_newdata),
            batch_size = as.integer(batch_size),
            verbose=as.integer(verbose))
        } else if(private$ml_framework=="pytorch"){
          if(torch$cuda$is_available()){
            device="cuda"
            dtype=torch$double
            model$to(device,dtype=dtype)
            model$eval()
            input=torch$from_numpy(np$array(real_newdata))
            predictions_prob<-model(input$to(device,dtype=dtype),
                                    predication_mode=TRUE)$detach()$cpu()$numpy()
          } else {
            device="cpu"
            dtype=torch$float
            model$to(device,dtype=dtype)
            model$eval()
            input=torch$from_numpy(np$array(real_newdata))
            predictions_prob<-model(input$to(device,dtype=dtype),
                                    predication_mode=TRUE)$detach()$numpy()
          }
        }


        #Add Column for the second characteristic
        predictions=vector(length = length(predictions_prob))
        predictions_binary_prob<-matrix(ncol=2,
                                        nrow=length(predictions_prob))

        for(i in 1:length(predictions_prob)){
          if(predictions_prob[i]>=0.5){
            predictions_binary_prob[i,1]=1-predictions_prob[i]
            predictions_binary_prob[i,2]=predictions_prob[i]
            predictions[i]=1
          } else {
            predictions_binary_prob[i,1]=1-predictions_prob[i]
            predictions_binary_prob[i,2]=predictions_prob[i]
            predictions[i]=0
          }
        }
        predictions_prob<-predictions_binary_prob
      }

      #Transforming predictions to target levels
      predictions<-as.character(as.vector(predictions))
      for(i in 0:(length(self$model_config$target_levels)-1)){
        predictions<-replace(x=predictions,
                             predictions==as.character(i),
                             values=self$model_config$target_levels[i+1])
      }

      #Transforming to a factor
      predictions=factor(predictions,levels = self$model_config$target_levels)

      colnames(predictions_prob)=self$model_config$target_levels
      predictions_prob<-as.data.frame(predictions_prob)
      predictions_prob$expected_category=predictions
      rownames(predictions_prob)=current_row_names

      return(predictions_prob)

    },
    #Check Embedding Model compatibility of the text embedding
    #'@description Method for checking if the provided text embeddings are
    #'created with the same \link{TextEmbeddingModel} as the classifier.
    #'@param text_embeddings Object of class \link{EmbeddedText}.
    #'@return \code{TRUE} if the underlying \link{TextEmbeddingModel} are the same.
    #'\code{FALSE} if the models differ.
    check_embedding_model=function(text_embeddings){
      if(("EmbeddedText" %in% class(text_embeddings))==FALSE){
        stop("text_embeddings is not of class EmbeddedText.")
      }

      embedding_model_config<-text_embeddings$get_model_info()
      for(check in names(embedding_model_config)){
        if(embedding_model_config[[check]]!=private$text_embedding_model$model[[check]]){
          return(FALSE)
        }
      }
      return(TRUE)
    },
    #General Information set and get--------------------------------------------
    #'@description Method for requesting the model information
    #'@return \code{list} of all relevant model information
    get_model_info=function(){
      return(list(
        model_license=private$model_info$model_license,
        model_name=private$model_info$model_name,
        model_name_root=private$model_info$model_name_root,
        model_label=private$model_info$model_label,
        model_date=private$model_info$model_date
      )
      )
    },
    #'@description Method for requesting the text embedding model information
    #'@return \code{list} of all relevant model information on the text embedding model
    #'underlying the classifier
    get_text_embedding_model=function(){
      return(private$text_embedding_model)
    },
    #---------------------------------------------------------------------------
    #'@description Method for setting publication information of the classifier
    #'@param authors List of authors.
    #'@param citation Free text citation.
    #'@param url URL of a corresponding homepage.
    #'@return Function does not return a value. It is used for setting the private
    #'members for publication information.
    set_publication_info=function(authors ,
                                  citation,
                                  url=NULL){

      private$publication_info$developed_by$authors<-authors
      private$publication_info$developed_by$citation<-citation
      private$publication_info$developed_by$url<-url

    },
    #--------------------------------------------------------------------------
    #'@description Method for requesting the bibliographic information of the classifier.
    #'@return \code{list} with all saved bibliographic information.
    get_publication_info=function(){
      return(private$publication_info)
    },
    #--------------------------------------------------------------------------
    #'@description Method for setting the license of the classifier.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return Function does not return a value. It is used for setting the private member for
    #'the software license of the model.
    set_software_license=function(license="GPL-3"){
      private$model_info$model_license<-license
    },
    #'@description Method for getting the license of the classifier.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return \code{string} representing the license for the software.
    get_software_license=function(){
      return(private$model_info$model_license)
    },
    #--------------------------------------------------------------------------
    #'@description Method for setting the license of the classifier's documentation.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return Function does not return a value. It is used for setting the private member for
    #'the documentation license of the model.
    set_documentation_license=function(license="CC BY-SA"){
      private$model_description$license<-license
    },
    #'@description Method for getting the license of the classifier's documentation.
    #'@param license \code{string} containing the abbreviation of the license or
    #'the license text.
    #'@return Returns the license as a \code{string}.
    get_documentation_license=function(){
      return(private$model_description$license)
    },
    #--------------------------------------------------------------------------
    #'@description Method for setting a description of the classifier.
    #'@param eng \code{string} A text describing the training of the learner,
    #'its theoretical and empirical background, and the different output labels
    #'in English.
    #'@param native \code{string} A text describing the training of the learner,
    #'its theoretical and empirical background, and the different output labels
    #'in the native language of the classifier.
    #'@param abstract_eng \code{string} A text providing a summary of the description
    #'in English.
    #'@param abstract_native \code{string} A text providing a summary of the description
    #'in the native language of the classifier.
    #'@param keywords_eng \code{vector} of keyword in English.
    #'@param keywords_native \code{vector} of keyword in the native language of the classifier.
    #'@return Function does not return a value. It is used for setting the private members for the
    #'description of the model.
    set_model_description=function(eng=NULL,
                                   native=NULL,
                                   abstract_eng=NULL,
                                   abstract_native=NULL,
                                   keywords_eng=NULL,
                                   keywords_native=NULL){
      if(!is.null(eng)){
        private$model_description$eng=eng
      }
      if(!is.null(native)){
        private$model_description$native=native
      }

      if(!is.null(abstract_eng)){
        private$model_description$abstract_eng=abstract_eng
      }
      if(!is.null(abstract_native)){
        private$model_description$abstract_native=abstract_native
      }

      if(!is.null(keywords_eng)){
        private$model_description$keywords_eng=keywords_eng
      }
      if(!is.null(keywords_native)){
        private$model_description$keywords_native=keywords_native
      }

    },
    #'@description Method for requesting the model description.
    #'@return \code{list} with the description of the classifier in English
    #'and the native language.
    get_model_description=function(){
      return(private$model_description)
    },
    #-------------------------------------------------------------------------
    #'@description Method for saving a model to 'Keras v3 format',
    #''tensorflow' SavedModel format or h5 format.
    #'@param dir_path \code{string()} Path of the directory where the model should be
    #'saved.
    #'@param save_format Format for saving the model. For 'tensorflow'/'keras' models
    #'\code{"keras"} for 'Keras v3 format',
    #'\code{"tf"} for SavedModel
    #'or \code{"h5"} for HDF5.
    #'For 'pytorch' models \code{"safetensors"} for 'safetensors' or
    #'\code{"pt"} for 'pytorch' via pickle.
    #'Use \code{"default"} for the standard format. This is keras for
    #''tensorflow'/'keras' models and safetensors for 'pytorch' models.
    #'@return Function does not return a value. It saves the model to disk.
    #'@importFrom utils write.csv
     save_model=function(dir_path,save_format="default"){
       if(private$ml_framework=="tensorflow"){
         if(save_format%in%c("safetensors","pt")){
           stop("'safetensors' and 'pt' are only supported for models based on
           pytorch.")
         }
       } else if(private$ml_framework=="pytorch"){
         if(save_format%in%c("keras","tf","h5")){
           stop("'keras','tf', and 'h5' are only supported for models based on
           tensorflow")
         }
       }

       if(save_format=="default"){
         if(private$ml_framework=="tensorflow"){
           save_format="keras"
         } else if(private$ml_framework=="pytorch"){
           save_format="safetensors"
         }
       }

       if(save_format=="safetensors" &
          reticulate::py_module_available("safetensors")==FALSE){
         warning("Python library 'safetensors' is not available. Using
                 standard save format for pytorch.")
         save_format="pt"
       }

       if(private$ml_framework=="tensorflow"){
         if(save_format=="keras"){
           extension=".keras"
         } else if(save_format=="tf"){
           extension=".tf"
         } else {
           extension=".h5"
         }
         file_path=paste0(dir_path,"/","model_data",extension)
         if(dir.exists(dir_path)==FALSE){
           dir.create(dir_path)
         }
         self$model$save(file_path)

       } else if(private$ml_framework=="pytorch"){
         self$model$to("cpu",dtype=torch$float)
         if(save_format=="safetensors"){
          file_path=paste0(dir_path,"/","model_data",".safetensors")
           if(dir.exists(dir_path)==FALSE){
             dir.create(dir_path)
           }
           safetensors$torch$save_model(model=self$model,filename=file_path)
         } else if (save_format=="pt"){
           file_path=paste0(dir_path,"/","model_data",".pt")
           if(dir.exists(dir_path)==FALSE){
             dir.create(dir_path)
           }
           torch$save(self$model$state_dict(),file_path)
         }
       }

       #Saving Sustainability Data
       sustain_matrix=t(as.matrix(unlist(private$sustainability)))
       write.csv(
         x=sustain_matrix,
         file=paste0(dir_path,"/","sustainability.csv"),
         row.names = FALSE
       )
    },
    #'@description Method for importing a model from 'Keras v3 format',
    #' 'tensorflow' SavedModel format or h5 format.
    #'@param dir_path \code{string()} Path of the directory where the model is
    #'saved.
    #'@param ml_framework \code{string} Determines the machine learning framework
    #'for using the model. Possible are \code{ml_framework="pytorch"} for 'pytorch',
    #'\code{ml_framework="tensorflow"} for 'tensorflow', and \code{ml_framework="auto"}.
    #'@return Function does not return a value. It is used to load the weights
    #'of a model.
    #'@importFrom utils compareVersion
    load_model=function(dir_path,
                        ml_framework="auto"){

      # Set the correct ml framework

      if((ml_framework %in%c("pytorch","tensorflow","auto","not_specified"))==FALSE){
        stop("ml_framework must be 'tensorflow', 'pytorch' or 'auto'.")
      }

      if(ml_framework=="not_specified"){
        stop("The global machine learning framework is not set. Please use
             aifeducation_config$set_global_ml_backend() directly after loading
             the library to set the global framework. ")
      }

      if(ml_framework!="auto"){
        private$ml_framework=ml_framework
      }

      #Load the model
      if(private$ml_framework=="tensorflow"){
        path=paste0(dir_path,"/","model_data",".keras")
        if(file.exists(paths = path)==TRUE){
          self$model<-keras$models$load_model(path)
        } else {
          path=paste0(dir_path,"/","model_data",".tf")
          if(dir.exists(paths = path)==TRUE){
            self$model<-keras$models$load_model(path)
          } else {
            path=paste0(dir_path,"/","model_data",".h5")
            if(file.exists(paths = path)==TRUE){
              self$model<-keras$models$load_model(paste0(dir_path,"/","model_data",".h5"))
            } else {
              stop("There is no compatible model file in the choosen directory.
                   Please check path. Please note that classifiers have to be loaded with
                   the same framework as during creation.")
            }
          }
        }
      } else if(private$ml_framework=="pytorch"){
          path_pt=paste0(dir_path,"/","model_data",".pt")
          path_safe_tensors=paste0(dir_path,"/","model_data",".safetensors")
          if(file.exists(path_safe_tensors)){
            self$model=private$create_model_pytorch(
              features=self$model_config$features ,
              times=self$model_config$times,
              hidden=self$model_config$hidden,
              rec=self$model_config$rec,
              intermediate_size=self$model_config$intermediate_size,
              attention_type=self$model_config$attention_type,
              repeat_encoder=self$model_config$repeat_encoder,
              dense_dropout=self$model_config$dense_dropout,
              rec_dropout=self$model_config$rec_dropout,
              encoder_dropout=self$model_config$encoder_dropout,
              add_pos_embedding=self$model_config$add_pos_embedding,
              self_attention_heads=self$model_config$self_attention_heads,
              target_levels=self$model_config$target_levels)
            safetensors$torch$load_model(model=self$model,filename=path_safe_tensors)
          } else {
            if(file.exists(paths = path_pt)==TRUE){
              self$model=private$create_model_pytorch(
                features=self$model_config$features ,
                times=self$model_config$times,
                hidden=self$model_config$hidden,
                rec=self$model_config$rec,
                intermediate_size=self$model_config$intermediate_size,
                attention_type=self$model_config$attention_type,
                repeat_encoder=self$model_config$repeat_encoder,
                dense_dropout=self$model_config$dense_dropout,
                rec_dropout=self$model_config$rec_dropout,
                encoder_dropout=self$model_config$encoder_dropout,
                add_pos_embedding=self$model_config$add_pos_embedding,
                self_attention_heads=self$model_config$self_attention_heads,
                target_levels=self$model_config$target_levels)

              self$model$load_state_dict(torch$load(path_pt))
            } else {
              stop("There is no compatible model file in the choosen directory.
                     Please check path. Please note that classifiers have to be loaded with
                     the same framework as during creation.")
            }
          }
        }
    },
    #---------------------------------------------------------------------------
    #'@description Method for requesting a summary of the R and python packages'
    #'versions used for creating the classifier.
    #'@return Returns a \code{list} containing the versions of the relevant
    #'R and python packages.
    get_package_versions=function(){
      return(
        list(r_package_versions=private$r_package_versions,
             py_package_versions=private$py_package_versions)
      )
    },
    #---------------------------------------------------------------------------
    #'@description Method for requesting a summary of tracked energy consumption
    #'during training and an estimate of the resulting CO2 equivalents in kg.
    #'@return Returns a \code{list} containing the tracked energy consumption,
    #'CO2 equivalents in kg, information on the tracker used, and technical
    #'information on the training infrastructure.
    get_sustainability_data=function(){
      return(private$sustainability)
    },
    #---------------------------------------------------------------------------
    #'@description Method for requesting the machine learning framework used
    #'for the classifier.
    #'@return Returns a \code{string} describing the machine learning framework used
    #'for the classifier
    get_ml_framework=function(){
      return(private$ml_framework)
    }
  ),
  private = list(
    ml_framework=NA,

    #General Information-------------------------------------------------------
    model_info=list(
      model_license=NA,
      model_name=NA,
      name_root=NA,
      model_label=NA,
      model_date=NA
    ),

    text_embedding_model=list(
      model=list(),
      times=NA,
      features=NA
    ),


    publication_info=list(
      developed_by=list(
        authors =NULL,
        citation=NULL,
        url=NULL
      )
    ),
    model_description=list(
      eng=NULL,
      native=NULL,
      abstract_eng=NULL,
      abstract_native=NULL,
      keywords_eng=NULL,
      keywords_native=NULL,
      license=NA
    ),

    r_package_versions=list(
      aifeducation=NA,
      smotefamily=NA,
      reticulate=NA
    ),

    py_package_versions=list(
      tensorflow=NA,
      torch=NA,
      keras=NA,
      numpy=NA
    ),

    sustainability=list(
      sustainability_tracked=FALSE,
      date=NA,
      sustainability_data=list(
        duration_sec=NA,
        co2eq_kg=NA,
        cpu_energy_kwh=NA,
        gpu_energy_kwh=NA,
        ram_energy_kwh=NA,
        total_energy_kwh=NA
      ),
      technical=list(
        tracker=NA,
        py_package_version=NA,

        cpu_count=NA,
        cpu_model=NA,

        gpu_count=NA,
        gpu_model=NA,

        ram_total_size=NA
      ),
      region=list(
        country_name=NA,
        country_iso_code=NA,
        region=NA
      )
    ),

    #Training Process----------------------------------------------------------
    init_weights=NULL,
    #--------------------------------------------------------------------------
    create_model_pytorch=function(features,
                                times,
                                hidden,
                                rec,
                                intermediate_size,
                                attention_type,
                                repeat_encoder,
                                dense_dropout,
                                rec_dropout,
                                recurrent_dropout,
                                encoder_dropout,
                                add_pos_embedding,
                                self_attention_heads,
                                target_levels){
      #Load Custom Pytorch Objects and Functions
      reticulate::py_run_file(system.file("python/pytorch_te_classifier.py",
                                          package = "aifeducation"))
      return(py$TextEmbeddingClassifier_PT(features=as.integer(features),
                                    times=as.integer(times),
                                    hidden=if(!is.null(hidden)){as.integer(hidden)}else{NULL},
                                    rec=if(!is.null(rec)){as.integer(rec)}else{NULL},
                                    intermediate_size=as.integer(intermediate_size),
                                    attention_type=attention_type,
                                    repeat_encoder=as.integer(repeat_encoder),
                                    dense_dropout=dense_dropout,
                                    rec_dropout=rec_dropout,
                                    encoder_dropout=encoder_dropout,
                                    add_pos_embedding=add_pos_embedding,
                                    self_attention_heads=as.integer(self_attention_heads),
                                    target_levels=target_levels))

    },
    #--------------------------------------------------------------------------
    create_model_keras=function(features,
                                times,
                                hidden,
                                rec,
                                embed_dim,
                                intermediate_size,
                                attention_type,
                                repeat_encoder,
                                dense_dropout,
                                rec_dropout,
                                recurrent_dropout,
                                encoder_dropout,
                                add_pos_embedding,
                                self_attention_heads,
                                target_levels,
                                act_fct_last,
                                name){
      #Load Custom Layers
      reticulate::py_run_file(system.file("python/keras_te_classifier.py",
                                          package = "aifeducation"))
      #Defining basic keras model
      layer_list=NULL

      if(is.null(rec)==TRUE){
        n_rec=0
      } else {
        n_rec=length(rec)
      }

      if(is.null(hidden)==TRUE){
        n_hidden=0
      } else {
        n_hidden=length(hidden)
      }

      #Adding Input Layer

      if(n_rec>0 | repeat_encoder>0){
        model_input<-keras$layers$Input(shape=list(as.integer(times),as.integer(features)),
                                        name="input_embeddings")
      } else {
        model_input<-keras$layers$Input(shape=as.integer(times*features),
                                        name="input_embeddings")
      }
      layer_list[1]<-list(model_input)

      #Adding a Mask-Layer
      if(n_rec>0 | repeat_encoder>0){
        masking_layer<-keras$layers$Masking(
          mask_value = 0.0,
          name="masking_layer",
          input_shape=c(times,features),
          trainable=FALSE)(layer_list[[length(layer_list)]])
        layer_list[length(layer_list)+1]<-list(masking_layer)

        if(add_pos_embedding==TRUE){
          positional_embedding<-py$AddPositionalEmbedding(sequence_length = as.integer(times),
                                                          name="add_positional_embedding")(layer_list[[length(layer_list)]])
          layer_list[length(layer_list)+1]<-list(positional_embedding)
        }

        norm_layer<-keras$layers$LayerNormalization(
          name = "normalizaion_layer")(layer_list[[length(layer_list)]])
        layer_list[length(layer_list)+1]<-list(norm_layer)

     } else {
        norm_layer<-keras$layers$BatchNormalization(
          name = "normalizaion_layer")(layer_list[[length(layer_list)]])
        layer_list[length(layer_list)+1]<-list(norm_layer)

      }

      if(repeat_encoder>0){
        for(r in 1:repeat_encoder){
          if(attention_type=="multihead"){
            layer_list[length(layer_list)+1]<-list(
              py$TransformerEncoder(embed_dim = as.integer(features),
                                    dense_dim= as.integer(intermediate_size),
                                    num_heads =as.integer(self_attention_heads),
                                    dropout_rate=encoder_dropout,
                                    name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
            )
          } else if(attention_type=="fourier"){
            layer_list[length(layer_list)+1]<-list(
              py$FourierEncoder(dense_dim=as.integer(intermediate_size),
                                dropout_rate=encoder_dropout,
                                name=paste0("encoder_",r))(layer_list[[length(layer_list)]])
            )
          }
        }
      }

      #Adding rec layer
      if(n_rec>0){
        for(i in 1:n_rec){
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Bidirectional(
              layer=keras$layers$GRU(
                units=as.integer(rec[i]),
                input_shape=list(times,features),
                return_sequences = TRUE,
                dropout = 0,
                recurrent_dropout = recurrent_dropout,
                activation = "tanh",
                name=paste0("gru_",i)),
              name=paste0("bidirectional_",i))(layer_list[[length(layer_list)]]))
          if (i!=n_rec){
            layer_list[length(layer_list)+1]<-list(
              keras$layers$Dropout(
                rate = rec_dropout,
                name=paste0("gru_dropout_",i))(layer_list[[length(layer_list)]]))
          }
        }
      }

        if(n_rec>0 | repeat_encoder>0){
          layer_list[length(layer_list)+1]<-list(
            keras$layers$GlobalAveragePooling1D(
              name="global_average_pooling")(layer_list[[length(layer_list)]]))
        }

      #Adding standard layer
      if(n_hidden>0){
        for(i in 1:n_hidden){
          layer_list[length(layer_list)+1]<-list(
            keras$layers$Dense(
              units = as.integer(hidden[i]),
              activation = "gelu",
              name=paste0("dense_",i))(layer_list[[length(layer_list)]]))

          if(i!=n_hidden){
            #Add Dropout_Layer
            layer_list[length(layer_list)+1]<-list(
              keras$layers$Dropout(
                rate = dense_dropout,
                name=paste0("dense_dropout_",i))(layer_list[[length(layer_list)]]))
          }
        }
      }

      #Adding final Layer
      if(length(target_levels)>2){
        #Multi Class
        layer_list[length(layer_list)+1]<-list(
          keras$layers$Dense(
            units = as.integer(length(target_levels)),
            activation = act_fct_last,
            name="output_categories")(layer_list[[length(layer_list)]]))
      } else {
        #Binary Class
        layer_list[length(layer_list)+1]<-list(
          keras$layers$Dense(
            units = as.integer(1),
            activation = act_fct_last,
            name="output_categories")(layer_list[[length(layer_list)]]))
      }

      #Creating Model
      model<-keras$Model(
        inputs = model_input,
        outputs = layer_list[length(layer_list)],
        name = name)

      self$model=model

    },
    basic_train=function(embedding_train,
                         target_train,
                         embedding_val,
                         target_val,
                         embedding_test=NULL,
                         target_test=NULL,
                         sample_weights=NULL,
                         reset_model=FALSE,
                         use_callback=TRUE,
                         epochs=100,
                         batch_size=32,
                         balance_class_weights=FALSE,
                         trace=TRUE,
                         keras_trace=2,
                         pytorch_trace=2,
                         dir_checkpoint,
                         shiny_app_active=FALSE){

      if("EmbeddedText" %in% class(embedding_train)){
        data_embedding_train=embedding_train$embeddings
      } else {
        data_embedding_train=embedding_train
      }
      if("EmbeddedText" %in% class(embedding_val)){
        data_embedding_val=embedding_val$embeddings
      } else {
        data_embedding_val=embedding_val
      }

      if(is.null(embedding_test)==FALSE){
        if("EmbeddedText" %in% class(embedding_test)){
          data_embedding_test=embedding_test$embeddings
        } else {
          data_embedding_test=embedding_test
        }
      }

      #Clear session to provide enough resources for computations
      if(private$ml_framework=="tensorflow"){
        keras$backend$clear_session()
      } else if(private$ml_framework=="pytorch"){
        if(torch$cuda$is_available()){
          torch$cuda$empty_cache()
        }
      }

      model<-self$model

      #load names and order of input variables and train_target levels
      variable_name_order<-self$model_config$input_variables
      target_levels_order<-self$model_config$target_levels

      #Ensuring the same encoding
      target_train<-factor(as.character(target_train),
                           levels = target_levels_order)
      target_val<-factor(as.character(target_val),
                          levels = target_levels_order)
      if(is.null(target_test)==FALSE){
        target_test<-factor(as.character(target_test),
                            levels = target_levels_order)
      }

      n_classes=length(levels(target_train))

      data_embedding_train<-data_embedding_train[,,variable_name_order]
      data_embedding_val<-data_embedding_val[,,variable_name_order]
      if(is.null(embedding_test)==FALSE){
        data_embedding_test<-data_embedding_test[,,variable_name_order]
      }


      #Transforming train_target for the use in keras.
      #That is switching characters to numeric
      target_train_transformed<-as.numeric(target_train)-1
      target_val_transformed<-as.numeric(target_val)-1
      if(is.null(target_test)==FALSE){
        target_test_transformed<-as.numeric(target_test)-1
      }

      #Generating class weights
      if(balance_class_weights==TRUE){
        abs_freq_classes=table(target_train_transformed)
        class_weights=as.vector(sum(abs_freq_classes)/(length(abs_freq_classes)*abs_freq_classes))
      } else {
        class_weights=rep(x=1,times=length(levels(target_train)))
      }

      #Convert Input data if the network cannot process sequential data
      if(is.null(self$model_config$rec)==TRUE){
        n_rec=0
      } else {
        n_rec=length(self$model_config$rec)
      }

      if(n_rec==0 & self$model_config$repeat_encoder==0){
        input_embeddings_train= array_to_matrix(data_embedding_train)
        input_embeddings_val=array_to_matrix(data_embedding_val)
        if(is.null(embedding_test)==FALSE){
          input_embeddings_test=array_to_matrix(data_embedding_test)
        }

      } else {
        input_embeddings_train=data_embedding_train
        input_embeddings_val=data_embedding_val
        if(is.null(embedding_test)==FALSE){
          input_embeddings_test=data_embedding_test
        }
      }

      n_categories=as.integer(length(levels(target_train)))


      if(length(target_levels_order)>2){
        #Multi Class
        output_categories_train=to_categorical_c(class_vector=target_train_transformed,
                                                 n_classes=n_categories)
        output_categories_val=to_categorical_c(class_vector=target_val_transformed,
                                                n_classes=n_categories)
        if(is.null(target_test)==FALSE){
          output_categories_test=to_categorical_c(class_vector=target_test_transformed,
                                                  n_classes=n_categories)
        }

      } else {
        #Binary Classification
        output_categories_train=target_train_transformed
        output_categories_val=target_val_transformed
        if(is.null(target_test)==FALSE){
          output_categories_test=target_test_transformed
        }
      }

      if(reset_model==TRUE){
        if(private$ml_framework=="tensorflow"){
          #According to the documentation the cloned model uses randomly
          #assigned weights
          model=tf$keras$models$clone_model(model)
          #model$set_weights(private$init_weights)
          #cat("Model reseted")
        } else if(private$ml_framework=="pytorch"){
          model=private$create_model_pytorch(
            features=self$model_config$features ,
            times=self$model_config$times,
            hidden=self$model_config$hidden,
            rec=self$model_config$rec,
            intermediate_size=self$model_config$intermediate_size,
            attention_type=self$model_config$attention_type,
            repeat_encoder=self$model_config$repeat_encoder,
            dense_dropout=self$model_config$dense_dropout,
            rec_dropout=self$model_config$rec_dropout,
            encoder_dropout=self$model_config$encoder_dropout,
            add_pos_embedding=self$model_config$add_pos_embedding,
            self_attention_heads=self$model_config$self_attention_heads,
            target_levels=self$model_config$target_levels)
        }
      }

      if(private$ml_framework=="tensorflow"){
        balanced_metric=py$BalancedAccuracy(n_classes = as.integer(length(self$model_config$target_levels)))
        if(self$model_config$optimizer=="adam"){
          model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$Adam(),
            metrics=c(self$model_config$metric,balanced_metric))
        } else if (self$model_config$optimizer=="rmsprop"){
          model$compile(
            loss = self$model_config$err_fct,
            optimizer=keras$optimizers$RMSprop(),
            metrics=c(self$model_config$metric,balanced_metric))
        }
      } else if(private$ml_framework=="pytorch"){
        #loss=torch$nn$CrossEntropyLoss(
        #  reduction="none",
        #  weight = torch$tensor(np$array(class_weights)))
        loss_fct_name="CrossEntropyLoss"
        if(self$model_config$optimizer=="adam"){
          optimizer="adam"
        } else if (self$model_config$optimizer=="rmsprop"){
          optimizer="rmsprop"
        }
      }

      if(dir.exists(paste0(dir_checkpoint,"/checkpoints"))==FALSE){
        if(trace==TRUE){
          message(paste(date(),"Creating Checkpoint Directory"))
        }
        dir.create(paste0(dir_checkpoint,"/checkpoints"))
      }

      if(private$ml_framework=="tensorflow"){
        if(use_callback==TRUE){
          callback=keras$callbacks$ModelCheckpoint(
            filepath = paste0(dir_checkpoint,"/checkpoints/best_weights.h5"),
            monitor = paste0("val_",self$model_config$balanced_metric),
            verbose = as.integer(min(keras_trace,1)),
            mode = "auto",
            save_best_only = TRUE,
            save_weights_only = TRUE)
        } else {
          callback=reticulate::py_none()
        }

        if(shiny_app_active==TRUE){
          reticulate::py_run_file(system.file("python/keras_callbacks.py",
                                              package = "aifeducation"))
          callback=list(callback,py$ReportAiforeducationShiny())
        }

        if(!is.null(sample_weights)){
          sample_weights=np$array(sample_weights)
        } else {
          sample_weights=reticulate::py_none()
        }

        history<-model$fit(
          verbose=as.integer(keras_trace),
          x=np$array(input_embeddings_train),
          y=np$array(output_categories_train),
          validation_data=reticulate::tuple(list(x_val=np$array(input_embeddings_val),
                                                 y_val=np$array(output_categories_val))),
          epochs = as.integer(epochs),
          batch_size = as.integer(batch_size),
          callbacks = callback,
          sample_weight=sample_weights,
          class_weight=reticulate::py_dict(keys = names(class_weights),values = class_weights))$history

        if(n_categories==2){
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$binary_accuracy,history$val_binary_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        } else {
          history=list(
            loss=rbind(history$loss,history$val_loss),
            accuracy=rbind(history$categorical_accuracy,history$val_categorical_accuracy),
            balanced_accuracy=rbind(history$balanced_accuracy,history$val_balanced_accuracy))
        }

        if(use_callback==TRUE){
          #message(paste(date(),"Load Weights From Best Checkpoint"))
          #model$load_weights(paste0(dir_checkpoint,"/checkpoints/best_weights.h5"))
          model$load_weights(paste0(dir_checkpoint,"/checkpoints/best_weights.h5"))
        }

        self$model=model
        self$model_config$input_variables<-variable_name_order
        self$model_config$target_levels<-target_levels_order

      } else if(private$ml_framework=="pytorch"){
        if(!is.null(sample_weights)){
          sample_weights=np$array(sample_weights)
        } else {
          sample_weights=np$array(rep.int(x=1,times = nrow(as.matrix(output_categories_train))))
        }

        pytorch_train_data=torch$utils$data$TensorDataset(
          torch$tensor(np$array(input_embeddings_train)),
          torch$tensor(np$array(as.matrix(output_categories_train))),
          torch$tensor(sample_weights))

        pytorch_val_data=torch$utils$data$TensorDataset(
          torch$tensor(np$array(input_embeddings_val)),
          torch$tensor(np$array(as.matrix(output_categories_val))))

        if((is.null(target_test)==FALSE) & (is.null(embedding_test)==FALSE)){
          pytorch_test_data=torch$utils$data$TensorDataset(
            torch$tensor(np$array(input_embeddings_test)),
            torch$tensor(np$array(as.matrix(output_categories_test))))
        } else {
          pytorch_test_data=NULL
        }

        history=py$TeClassifierTrain_PT(
          model=model,
          loss_fct_name=loss_fct_name,
          optimizer_method=optimizer,
          epochs=as.integer(epochs),
          trace=as.integer(pytorch_trace),
          use_callback=use_callback,
          batch_size=as.integer(batch_size),
          train_data=pytorch_train_data,
          val_data=pytorch_val_data,
          test_data=pytorch_test_data,
          #filepath=paste0(dir_checkpoint,"/checkpoints/best_weights.pt"),
          filepath=paste0(dir_checkpoint,"/checkpoints/best_weights.safetensors"),
          n_classes=n_classes,
          shiny_app_active=shiny_app_active,
          class_weights=torch$tensor(np$array(class_weights)))

        self$model=model
        self$model_config$input_variables<-variable_name_order
        self$model_config$target_levels<-target_levels_order
      }

      #Provide rownames for the history
      for(i in 1:length(history)){
          if(!is.null(history[[i]])){
              if(nrow(history[[i]])==2){
                rownames(history[[i]])=c("train","val")
              } else {
                rownames(history[[i]])=c("train","val","test")
              }
          }
      }


      return(history)
    },
    #--------------------------------------------------------------------------
    #Method for summarizing sustainability data for this classifier
    #List for results must correspond to the private fields of the classifier
    summarize_tracked_sustainability=function(sustainability_tracker){

      results<-list(
        sustainability_tracked=TRUE,
        sustainability_data=list(
          co2eq_kg=sustainability_tracker$final_emissions_data$emissions,
          cpu_energy_kwh=sustainability_tracker$final_emissions_data$cpu_energy,
          gpu_energy_kwh=sustainability_tracker$final_emissions_data$gpu_energy,
          ram_energy_kwh=sustainability_tracker$final_emissions_data$ram_energy,
          total_energy_kwh=sustainability_tracker$final_emissions_data$energy_consumed
        ),
        technical=list(
          tracker="codecarbon",
          py_package_version=codecarbon$"__version__",

          cpu_count=sustainability_tracker$final_emissions_data$cpu_count,
          cpu_model=sustainability_tracker$final_emissions_data$cpu_model,

          gpu_count=sustainability_tracker$final_emissions_data$gpu_count,
          gpu_model=sustainability_tracker$final_emissions_data$gpu_model,

          ram_total_size=sustainability_tracker$final_emissions_data$ram_total_size
        ),
        region=list(
          country_name=sustainability_tracker$final_emissions_data$country_name,
          country_iso_code=sustainability_tracker$final_emissions_data$country_iso_code,
          region=sustainability_tracker$final_emissions_data$region
        )
      )
      return(results)
    }
  )
)

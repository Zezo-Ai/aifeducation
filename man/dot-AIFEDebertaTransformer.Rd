% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dotAIFEDebertaTransformer.R
\name{.AIFEDebertaTransformer}
\alias{.AIFEDebertaTransformer}
\title{Child \code{R6} class for creation and training of \code{DeBERTa-V2} transformers}
\description{
This class has the following methods:
\itemize{
\item \code{create}: creates a new transformer based on \code{DeBERTa-V2}.
\item \code{train}: trains and fine-tunes a \code{DeBERTa-V2} model.
}
}
\note{
For this model a \code{WordPiece} tokenizer is created. The standard implementation of \code{DeBERTa} version 2 from
HuggingFace uses a \code{SentencePiece} tokenizer. Thus, please use \code{AutoTokenizer} from the \code{transformers} library to
work with this model.
}
\section{Create}{
 New models can be created using the \code{.AIFEDebertaTransformer$create} method.
}

\section{Train}{
 To train the model, pass the directory of the model to the method \code{.AIFEDebertaTransformer$train}.

Pre-Trained models which can be fine-tuned with this function are available at \url{https://huggingface.co/}.

Training of this model makes use of dynamic masking.
}

\references{
He, P., Liu, X., Gao, J. & Chen, W. (2020). DeBERTa: Decoding-enhanced BERT with Disentangled Attention.
\doi{10.48550/arXiv.2006.03654}

Hugging Face documentatio
\itemize{
\item \url{https://huggingface.co/docs/transformers/model_doc/deberta-v2}
\item \url{https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM}
\item \url{https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM}
}
}
\seealso{
Other Transformers for developers: 
\code{\link{.AIFEBaseTransformer}},
\code{\link{.AIFEBertTransformer}},
\code{\link{.AIFEFunnelTransformer}},
\code{\link{.AIFELongformerTransformer}},
\code{\link{.AIFEMpnetTransformer}},
\code{\link{.AIFERobertaTransformer}},
\code{\link{.AIFETrObj}}
}
\concept{Transformers for developers}
\section{Super class}{
\code{\link[aifeducation:.AIFEBaseTransformer]{aifeducation::.AIFEBaseTransformer}} -> \code{.AIFEDebertaTransformer}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-.AIFEDebertaTransformer-new}{\code{.AIFEDebertaTransformer$new()}}
\item \href{#method-.AIFEDebertaTransformer-create}{\code{.AIFEDebertaTransformer$create()}}
\item \href{#method-.AIFEDebertaTransformer-train}{\code{.AIFEDebertaTransformer$train()}}
\item \href{#method-.AIFEDebertaTransformer-clone}{\code{.AIFEDebertaTransformer$clone()}}
}
}
\if{html}{\out{
<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEDebertaTransformer-new"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEDebertaTransformer-new}{}}}
\subsection{Method \code{new()}}{
Creates a new transformer based on \code{DeBERTa-V2} and sets the title.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEDebertaTransformer$new()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEDebertaTransformer-create"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEDebertaTransformer-create}{}}}
\subsection{Method \code{create()}}{
This method creates a transformer configuration based on the \code{DeBERTa-V2} base architecture and a
vocabulary based on the \code{SentencePiece} tokenizer using the python \code{transformers} and \code{tokenizers} libraries.

This method adds the following \emph{'dependent' parameters} to the base class's inherited \code{params} list:
\itemize{
\item \code{vocab_do_lower_case}
\item \code{num_hidden_layer}
}
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEDebertaTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 128100,
  vocab_do_lower_case = FALSE,
  max_position_embeddings = 512,
  hidden_size = 1536,
  num_hidden_layer = 24,
  num_attention_heads = 24,
  intermediate_size = 6144,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\itemize{
\item \code{ml_framework = "tensorflow"}: for 'tensorflow'.
\item \code{ml_framework = "pytorch"}: for 'pytorch'.
}}

\item{\code{model_dir}}{\code{string} Path to the directory where the model should be saved.}

\item{\code{text_dataset}}{Object of class \link{LargeDataSetForText}.}

\item{\code{vocab_size}}{\code{int} Size of the vocabulary.}

\item{\code{vocab_do_lower_case}}{\code{bool} \code{TRUE} if all words/tokens should be lower case.}

\item{\code{max_position_embeddings}}{\code{int} Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.}

\item{\code{hidden_size}}{\code{int} Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.}

\item{\code{num_hidden_layer}}{\code{int} Number of hidden layers.}

\item{\code{num_attention_heads}}{\code{int} Number of attention heads.}

\item{\code{intermediate_size}}{\code{int} Number of neurons in the intermediate layer of the attention mechanism.}

\item{\code{hidden_act}}{\code{string} Name of the activation function.}

\item{\code{hidden_dropout_prob}}{\code{double} Ratio of dropout.}

\item{\code{attention_probs_dropout_prob}}{\code{double} Ratio of dropout for attention probabilities.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{\code{string} Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information \url{https://mlco2.github.io/codecarbon/parameters.html}.}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power usage.}

\item{\code{trace}}{\code{bool} \code{TRUE} if information about the progress should be printed to the console.}

\item{\code{pytorch_safetensors}}{\code{bool} Only relevant for pytorch models.
\itemize{
\item \code{TRUE}: a 'pytorch' model is saved in safetensors format.
\item \code{FALSE} (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
}}

\item{\code{log_dir}}{Path to the directory where the log files should be saved.}

\item{\code{log_write_interval}}{\code{int} Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if \code{log_dir} is not \code{NULL}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEDebertaTransformer-train"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEDebertaTransformer-train}{}}}
\subsection{Method \code{train()}}{
This method can be used to train or fine-tune a transformer based on \code{DeBERTa-V2} architecture with
the help of the python libraries \code{transformers}, \code{datasets}, and \code{tokenizers}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEDebertaTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.03,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\itemize{
\item \code{ml_framework = "tensorflow"}: for 'tensorflow'.
\item \code{ml_framework = "pytorch"}: for 'pytorch'.
}}

\item{\code{output_dir}}{\code{string} Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.}

\item{\code{model_dir_path}}{\code{string} Path to the directory where the original model is stored.}

\item{\code{text_dataset}}{Object of class \link{LargeDataSetForText}.}

\item{\code{p_mask}}{\code{double} Ratio that determines the number of words/tokens used for masking.}

\item{\code{whole_word}}{\code{bool}
\itemize{
\item \code{TRUE}: whole word masking should be applied.
\item \code{FALSE}: token masking is used.
}}

\item{\code{val_size}}{\code{double} Ratio that determines the amount of token chunks used for validation.}

\item{\code{n_epoch}}{\code{int} Number of epochs for training.}

\item{\code{batch_size}}{\code{int} Size of batches.}

\item{\code{chunk_size}}{\code{int} Size of every chunk for training.}

\item{\code{full_sequences_only}}{\code{bool} \code{TRUE} for using only chunks with a sequence length equal to \code{chunk_size}.}

\item{\code{min_seq_len}}{\code{int} Only relevant if \code{full_sequences_only = FALSE}. Value determines the minimal sequence length included in
training process.}

\item{\code{learning_rate}}{\code{double} Learning rate for adam optimizer.}

\item{\code{n_workers}}{\code{int} Number of workers. Only relevant if \code{ml_framework = "tensorflow"}.}

\item{\code{multi_process}}{\code{bool} \code{TRUE} if multiple processes should be activated. Only relevant if \code{ml_framework = "tensorflow"}.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{\code{string} Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information \url{https://mlco2.github.io/codecarbon/parameters.html}.}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power usage.}

\item{\code{trace}}{\code{bool} \code{TRUE} if information about the progress should be printed to the console.}

\item{\code{keras_trace}}{\code{int}
\itemize{
\item \code{keras_trace = 0}: does not print any information about the training process from keras on the console.
\item \code{keras_trace = 1}: prints a progress bar.
\item \code{keras_trace = 2}: prints one line of information for every epoch. Only relevant if \code{ml_framework = "tensorflow"}.
}}

\item{\code{pytorch_trace}}{\code{int}
\itemize{
\item \code{pytorch_trace = 0}: does not print any information about the training process from pytorch on the console.
\item \code{pytorch_trace = 1}: prints a progress bar.
}}

\item{\code{pytorch_safetensors}}{\code{bool} Only relevant for pytorch models.
\itemize{
\item \code{TRUE}: a 'pytorch' model is saved in safetensors format.
\item \code{FALSE} (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
}}

\item{\code{log_dir}}{Path to the directory where the log files should be saved.}

\item{\code{log_write_interval}}{\code{int} Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if \code{log_dir} is not \code{NULL}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEDebertaTransformer-clone"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEDebertaTransformer-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEDebertaTransformer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}

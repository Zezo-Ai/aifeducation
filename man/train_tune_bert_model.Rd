% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/transformer_bert.R
\name{train_tune_bert_model}
\alias{train_tune_bert_model}
\title{Function for training and fine-tuning a BERT model}
\usage{
train_tune_bert_model(
  output_dir,
  model_dir_path,
  raw_texts,
  aug_vocab_by = 100,
  p_mask = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  learning_rate = 0.003,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1
)
}
\arguments{
\item{output_dir}{\code{string} Path to the directory where the final model
should be saved. If the directory does not exist, it will be created.}

\item{model_dir_path}{\code{string} Path to the directory where the original
model is stored.}

\item{raw_texts}{\code{vector} containing the raw texts for training.}

\item{aug_vocab_by}{\code{int} Number of entries for extending the current
vocabulary. See notes for more details}

\item{p_mask}{\code{double} Ratio determining the number of words/tokens for masking.}

\item{whole_word}{\code{bool} \code{TRUE} if whole word masking should be applied.
If \code{FALSE} token masking is used.}

\item{val_size}{\code{double} Ratio determining the amount of token chunks used for
validation.}

\item{n_epoch}{\code{int} Number of epochs for training.}

\item{batch_size}{\code{int} Size of batches.}

\item{chunk_size}{\code{int} Size of every chunk for training.}

\item{learning_rate}{\code{double} Learning rate for adam optimizer.}

\item{n_workers}{\code{int} Number of workers.}

\item{multi_process}{\code{bool} \code{TRUE} if multiple processes should be activated.}

\item{sustain_track}{\code{bool} If \code{TRUE} energy consumption is tracked
during training via the python library codecarbon.}

\item{sustain_iso_code}{\code{string} ISO code (Alpha-3-Code) for the country. This variable
must be set if sustainability should be tracked. A list can be found on
Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{sustain_region}{Region within a country. Only available for USA and
Canada See the documentation of codecarbon for more information.
\url{https://mlco2.github.io/codecarbon/parameters.html}}

\item{sustain_interval}{\code{integer} Interval in seconds for measuring power
usage.}

\item{trace}{\code{bool} \code{TRUE} if information on the progress should be printed
to the console.}

\item{keras_trace}{\code{int} \code{keras_trace=0} does not print any
information about the training process from keras on the console.
\code{keras_trace=1} prints a progress bar. \code{keras_trace=2} prints
one line of information for every epoch.}
}
\value{
This function does not return an object. Instead the trained or fine-tuned
model is saved to disk.
}
\description{
This function can be used to train or fine-tune a transformer
based on BERT architecture with the help of the python libraries 'transformers',
'datasets', and 'tokenizers'.
}
\note{
if \code{aug_vocab_by > 0} the raw text is used for training a WordPiece
tokenizer. At the end of this process, additional entries are added to the vocabulary
that are not part of the original vocabulary. This is in an experimental state.

Pre-Trained models which can be fine-tuned with this function are available
at \url{https://huggingface.co/}.

New models can be created via the function \link{create_bert_model}.

Training of the model makes use of dynamic masking in contrast to the
original paper where static masking was applied.
}
\references{
Devlin, J., Chang, M.â€‘W., Lee, K., & Toutanova, K. (2019). BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding. In J. Burstein, C. Doran, & T. Solorio (Eds.),
Proceedings of the 2019 Conference of the North (pp. 4171--4186).
Association for Computational Linguistics.
\doi{10.18653/v1/N19-1423}

Hugging Face documentation
\url{https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForMaskedLM}
}
\seealso{
Other Transformer: 
\code{\link{create_bert_model}()},
\code{\link{create_longformer_model}()},
\code{\link{create_roberta_model}()},
\code{\link{train_tune_longformer_model}()},
\code{\link{train_tune_roberta_model}()}
}
\concept{Transformer}

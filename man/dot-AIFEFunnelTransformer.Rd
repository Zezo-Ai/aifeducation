% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dotAIFEFunnelTransformer.R
\name{.AIFEFunnelTransformer}
\alias{.AIFEFunnelTransformer}
\title{Child \code{R6} class for creation and training of \code{Funnel} transformers}
\description{
This class has the following methods:
\itemize{
\item \code{create}: creates a new transformer based on \code{Funnel}.
\item \code{train}: trains and fine-tunes a \code{Funnel} model.
}
}
\note{
The model uses a configuration with \code{truncate_seq = TRUE} to avoid implementation problems with tensorflow.

This model uses a \code{WordPiece} tokenizer like \code{BERT} and can be trained with whole word masking. The transformer
library may display a warning, which can be ignored.
}
\section{Create}{
 New models can be created using the \code{.AIFEFunnelTransformer$create} method.

Model is created with \code{separete_cls = TRUE}, \code{truncate_seq = TRUE}, and \code{pool_q_only = TRUE}.
}

\section{Train}{
 To train the model, pass the directory of the model to the method \code{.AIFEFunnelTransformer$train}.

Pre-Trained models which can be fine-tuned with this function are available at \url{https://huggingface.co/}.

Training of the model makes use of dynamic masking.
}

\references{
Dai, Z., Lai, G., Yang, Y. & Le, Q. V. (2020). Funnel-Transformer: Filtering out Sequential Redundancy
for Efficient Language Processing. \doi{10.48550/arXiv.2006.03236}

Hugging Face documentation
\itemize{
\item \url{https://huggingface.co/docs/transformers/model_doc/funnel#funnel-transformer}
\item \url{https://huggingface.co/docs/transformers/model_doc/funnel#transformers.FunnelModel}
\item \url{https://huggingface.co/docs/transformers/model_doc/funnel#transformers.TFFunnelModel}
}
}
\seealso{
Other Transformers for developers: 
\code{\link{.AIFEBaseTransformer}},
\code{\link{.AIFEBertTransformer}},
\code{\link{.AIFEDebertaTransformer}},
\code{\link{.AIFELongformerTransformer}},
\code{\link{.AIFEMpnetTransformer}},
\code{\link{.AIFERobertaTransformer}},
\code{\link{.AIFETrObj}}
}
\concept{Transformers for developers}
\section{Super class}{
\code{\link[aifeducation:.AIFEBaseTransformer]{aifeducation::.AIFEBaseTransformer}} -> \code{.AIFEFunnelTransformer}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-.AIFEFunnelTransformer-new}{\code{.AIFEFunnelTransformer$new()}}
\item \href{#method-.AIFEFunnelTransformer-create}{\code{.AIFEFunnelTransformer$create()}}
\item \href{#method-.AIFEFunnelTransformer-train}{\code{.AIFEFunnelTransformer$train()}}
\item \href{#method-.AIFEFunnelTransformer-clone}{\code{.AIFEFunnelTransformer$clone()}}
}
}
\if{html}{\out{
<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEFunnelTransformer-new"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEFunnelTransformer-new}{}}}
\subsection{Method \code{new()}}{
Creates a new transformer based on \code{Funnel} and sets the title.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEFunnelTransformer$new()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
This method returns nothing.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEFunnelTransformer-create"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEFunnelTransformer-create}{}}}
\subsection{Method \code{create()}}{
This method creates a transformer configuration based on the \code{Funnel} transformer base architecture
and a vocabulary based on \code{WordPiece} using the python \code{transformers} and \code{tokenizers} libraries.

This method adds the following \emph{'dependent' parameters} to the base class's inherited \code{params} list:
\itemize{
\item \code{vocab_do_lower_case}
\item \code{target_hidden_size}
\item \code{block_sizes}
\item \code{num_decoder_layers}
\item \code{pooling_type}
\item \code{activation_dropout}
}
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEFunnelTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 30522,
  vocab_do_lower_case = FALSE,
  max_position_embeddings = 512,
  hidden_size = 768,
  target_hidden_size = 64,
  block_sizes = c(4, 4, 4),
  num_attention_heads = 12,
  intermediate_size = 3072,
  num_decoder_layers = 2,
  pooling_type = "mean",
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  activation_dropout = 0,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\itemize{
\item \code{ml_framework = "tensorflow"}: for 'tensorflow'.
\item \code{ml_framework = "pytorch"}: for 'pytorch'.
}}

\item{\code{model_dir}}{\code{string} Path to the directory where the model should be saved.}

\item{\code{text_dataset}}{Object of class \link{LargeDataSetForText}.}

\item{\code{vocab_size}}{\code{int} Size of the vocabulary.}

\item{\code{vocab_do_lower_case}}{\code{bool} \code{TRUE} if all words/tokens should be lower case.}

\item{\code{max_position_embeddings}}{\code{int} Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.}

\item{\code{hidden_size}}{\code{int} Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.}

\item{\code{target_hidden_size}}{\code{int} Number of neurons in the final layer. This parameter determines the dimensionality of the resulting text
embedding.}

\item{\code{block_sizes}}{\code{vector} of \code{int} determining the number and sizes of each block.}

\item{\code{num_attention_heads}}{\code{int} Number of attention heads.}

\item{\code{intermediate_size}}{\code{int} Number of neurons in the intermediate layer of the attention mechanism.}

\item{\code{num_decoder_layers}}{\code{int} Number of decoding layers.}

\item{\code{pooling_type}}{\code{string} Type of pooling.
\itemize{
\item \code{"mean"} for pooling with mean.
\item \code{"max"} for pooling with maximum values.
}}

\item{\code{hidden_act}}{\code{string} Name of the activation function.}

\item{\code{hidden_dropout_prob}}{\code{double} Ratio of dropout.}

\item{\code{attention_probs_dropout_prob}}{\code{double} Ratio of dropout for attention probabilities.}

\item{\code{activation_dropout}}{\code{float} Dropout probability between the layers of the feed-forward blocks.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{\code{string} Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information \url{https://mlco2.github.io/codecarbon/parameters.html}.}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power usage.}

\item{\code{trace}}{\code{bool} \code{TRUE} if information about the progress should be printed to the console.}

\item{\code{pytorch_safetensors}}{\code{bool} Only relevant for pytorch models.
\itemize{
\item \code{TRUE}: a 'pytorch' model is saved in safetensors format.
\item \code{FALSE} (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
}}

\item{\code{log_dir}}{Path to the directory where the log files should be saved.}

\item{\code{log_write_interval}}{\code{int} Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if \code{log_dir} is not \code{NULL}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEFunnelTransformer-train"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEFunnelTransformer-train}{}}}
\subsection{Method \code{train()}}{
This method can be used to train or fine-tune a transformer based on \code{Funnel} Transformer
architecture with the help of the python libraries \code{transformers}, \code{datasets}, and \code{tokenizers}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEFunnelTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.003,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{ml_framework}}{\code{string} Framework to use for training and inference.
\itemize{
\item \code{ml_framework = "tensorflow"}: for 'tensorflow'.
\item \code{ml_framework = "pytorch"}: for 'pytorch'.
}}

\item{\code{output_dir}}{\code{string} Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.}

\item{\code{model_dir_path}}{\code{string} Path to the directory where the original model is stored.}

\item{\code{text_dataset}}{Object of class \link{LargeDataSetForText}.}

\item{\code{p_mask}}{\code{double} Ratio that determines the number of words/tokens used for masking.}

\item{\code{whole_word}}{\code{bool}
\itemize{
\item \code{TRUE}: whole word masking should be applied.
\item \code{FALSE}: token masking is used.
}}

\item{\code{val_size}}{\code{double} Ratio that determines the amount of token chunks used for validation.}

\item{\code{n_epoch}}{\code{int} Number of epochs for training.}

\item{\code{batch_size}}{\code{int} Size of batches.}

\item{\code{chunk_size}}{\code{int} Size of every chunk for training.}

\item{\code{full_sequences_only}}{\code{bool} \code{TRUE} for using only chunks with a sequence length equal to \code{chunk_size}.}

\item{\code{min_seq_len}}{\code{int} Only relevant if \code{full_sequences_only = FALSE}. Value determines the minimal sequence length included in
training process.}

\item{\code{learning_rate}}{\code{double} Learning rate for adam optimizer.}

\item{\code{n_workers}}{\code{int} Number of workers. Only relevant if \code{ml_framework = "tensorflow"}.}

\item{\code{multi_process}}{\code{bool} \code{TRUE} if multiple processes should be activated. Only relevant if \code{ml_framework = "tensorflow"}.}

\item{\code{sustain_track}}{\code{bool} If \code{TRUE} energy consumption is tracked during training via the python library codecarbon.}

\item{\code{sustain_iso_code}}{\code{string} ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes}.}

\item{\code{sustain_region}}{\code{string} Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information \url{https://mlco2.github.io/codecarbon/parameters.html}.}

\item{\code{sustain_interval}}{\code{integer} Interval in seconds for measuring power usage.}

\item{\code{trace}}{\code{bool} \code{TRUE} if information about the progress should be printed to the console.}

\item{\code{keras_trace}}{\code{int}
\itemize{
\item \code{keras_trace = 0}: does not print any information about the training process from keras on the console.
\item \code{keras_trace = 1}: prints a progress bar.
\item \code{keras_trace = 2}: prints one line of information for every epoch. Only relevant if \code{ml_framework = "tensorflow"}.
}}

\item{\code{pytorch_trace}}{\code{int}
\itemize{
\item \code{pytorch_trace = 0}: does not print any information about the training process from pytorch on the console.
\item \code{pytorch_trace = 1}: prints a progress bar.
}}

\item{\code{pytorch_safetensors}}{\code{bool} Only relevant for pytorch models.
\itemize{
\item \code{TRUE}: a 'pytorch' model is saved in safetensors format.
\item \code{FALSE} (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
}}

\item{\code{log_dir}}{Path to the directory where the log files should be saved.}

\item{\code{log_write_interval}}{\code{int} Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if \code{log_dir} is not \code{NULL}.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.AIFEFunnelTransformer-clone"></a>}}
\if{latex}{\out{\hypertarget{method-.AIFEFunnelTransformer-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{.AIFEFunnelTransformer$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
